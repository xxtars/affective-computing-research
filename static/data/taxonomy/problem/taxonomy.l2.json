[
  {
    "topic_id": 0,
    "size": 73,
    "keywords": [
      "dependency modeling",
      "dependency",
      "temporal",
      "modeling",
      "context",
      "modeling temporal",
      "longterm",
      "modeling longterm",
      "dynamic",
      "interval localization"
    ],
    "examples": [
      "dynamic sequence analysis",
      "dynamic sequence analysis",
      "spatiotemporal pattern analysis",
      "dynamic expression analysis",
      "temporal alignment of expressions",
      "sequential pattern recognition",
      "utterance independent modeling",
      "temporal alignment of expressions"
    ],
    "topic_fingerprint": "e43ce1bd3d87d386a7f819af7cdefbf9e7f9988d0299621027032c1bb23c6ae8",
    "l2_name": "Temporal Dependency Modeling",
    "definition": "The computational task of capturing and representing dynamic, long-term, and contextual dependencies within sequential or spatiotemporal data.",
    "aliases": [
      "temporal dynamics modeling",
      "dynamic sequence analysis",
      "long-term dependency modeling",
      "spatiotemporal dependency modeling",
      "sequential pattern recognition",
      "temporal alignment",
      "contextual temporal modeling",
      "dynamic expression analysis",
      "modality dependency modeling",
      "utterance independent modeling"
    ]
  },
  {
    "topic_id": 1,
    "size": 69,
    "keywords": [
      "conversion",
      "emotional speech",
      "speech",
      "speakerindependent",
      "expressive",
      "prosody",
      "speakerindependent speech",
      "emotional voice",
      "voice conversion",
      "voice"
    ],
    "examples": [
      "emotional speech classification",
      "emotional prosody generation",
      "speaker-independent speech emotion recognition",
      "prosodic feature modeling",
      "speaker-independent speech emotion recognition",
      "text versus speech emotion discrepancy",
      "speaker-independent emotion classification",
      "exclamatory speech prosody modeling"
    ],
    "topic_fingerprint": "3b6c4943f53bf104e5ddf1cd9f2dfc70a24059d4fce1dccb2c16cefbfb2eafdd",
    "l2_name": "Speech Emotion Recognition and Synthesis",
    "definition": "This field focuses on the analysis, classification, and generation of emotional content in speech through prosodic modeling and voice conversion techniques.",
    "aliases": [
      "SER",
      "Emotional Speech Processing",
      "Expressive Voice Conversion",
      "Prosody Modeling",
      "Speaker-Independent Emotion Recognition",
      "Emotional Prosody Generation",
      "Expressive Speech Synthesis"
    ]
  },
  {
    "topic_id": 2,
    "size": 68,
    "keywords": [
      "multimodal sentiment",
      "analysis multimodal",
      "sentiment analysis",
      "sentiment",
      "multimodal",
      "analysis",
      "incomplete multimodal",
      "incomplete",
      "robust multimodal",
      "prediction multimodal"
    ],
    "examples": [
      "multiword expression analysis",
      "multidimensional sentiment classification",
      "multimodal sentiment prediction",
      "multimodal sentiment analysis",
      "multimodal sentiment prediction",
      "multimodal news sentiment recognition",
      "multi-modal sentiment analysis",
      "multi-modal sentiment analysis"
    ],
    "topic_fingerprint": "d1626a55461961a2512c8cf18659b177edcb7a56cc8142cd6deacf93d61619ec",
    "l2_name": "Multimodal Sentiment Analysis",
    "definition": "The computational task of detecting, extracting, and classifying sentiment or emotion by integrating information from multiple modalities such as text, audio, and visual cues.",
    "aliases": [
      "multimodal sentiment prediction",
      "multimodal sentiment classification",
      "multi-modal sentiment analysis",
      "robust multimodal sentiment analysis",
      "incomplete multimodal sentiment analysis",
      "multimodal news sentiment recognition",
      "multidimensional sentiment classification"
    ]
  },
  {
    "topic_id": 3,
    "size": 62,
    "keywords": [
      "task emotion",
      "classification task",
      "reasoning",
      "emotion classification",
      "emotion reasoning",
      "explanation",
      "accuracy",
      "benchmarking",
      "causal",
      "reasoning emotion"
    ],
    "examples": [
      "emotion classification task",
      "emotion classification task",
      "emotion classification task",
      "emotion data representation standards",
      "causal link identification",
      "emotion regression",
      "emotion feature evaluation",
      "image emotion perception task"
    ],
    "topic_fingerprint": "9490947ad24d08a2d13ee78cfbf8f563461bff0b9c7a713005a5bbdf80a8dad6",
    "l2_name": "Emotion Classification",
    "definition": "The task of identifying, categorizing, and reasoning about emotional states within data to determine specific emotion labels or causal links.",
    "aliases": [
      "emotion classification task",
      "emotion reasoning",
      "emotion causation discovery",
      "causal link identification",
      "emotion feature evaluation",
      "image emotion perception task",
      "emotion regression",
      "mean emotion extraction",
      "emotion classification challenge"
    ]
  },
  {
    "topic_id": 4,
    "size": 60,
    "keywords": [
      "feature",
      "feature extraction",
      "discriminative",
      "extraction",
      "feature selection",
      "selection",
      "acoustic",
      "acoustic feature",
      "emotional feature",
      "feature representation"
    ],
    "examples": [
      "acoustic feature realization",
      "feature representation generation",
      "feature importance analysis",
      "robust emotion feature selection",
      "acoustic feature representation generation",
      "acoustic feature selection",
      "feature selection challenge",
      "high-dimensional feature reduction"
    ],
    "topic_fingerprint": "6e80b96cdf6b2c2062b9ff6da872779cc67926e791f2c5e064339be4895b656a",
    "l2_name": "Feature Selection and Extraction",
    "definition": "The process of identifying, generating, and reducing discriminative acoustic or emotional features to mitigate high dimensionality and improve representation quality.",
    "aliases": [
      "feature extraction",
      "feature selection",
      "discriminative feature selection",
      "acoustic feature extraction",
      "emotional feature selection",
      "feature representation generation",
      "high-dimensional feature reduction",
      "curse of dimensionality mitigation"
    ]
  },
  {
    "topic_id": 5,
    "size": 59,
    "keywords": [
      "sentiment",
      "sentiment classification",
      "text sentiment",
      "sentiment analysis",
      "text",
      "task sentiment",
      "classification",
      "discrete sentiment",
      "sentence",
      "short text"
    ],
    "examples": [
      "semantic polarity detection",
      "sentiment inversion handling",
      "metaphor polarity detection",
      "sentiment orientation analysis",
      "social sentiment detection",
      "topic sentiment analysis",
      "sentence sentiment classification",
      "short text sentiment classification"
    ],
    "topic_fingerprint": "8b0a20afbe7a1e9511d1e20fb6896b271339ebebf40f184edec73dc4e1fb0705",
    "l2_name": "Sentiment Analysis",
    "definition": "The computational task of identifying, extracting, and classifying the subjective polarity or emotional orientation expressed in text.",
    "aliases": [
      "Opinion Mining",
      "Sentiment Classification",
      "Semantic Polarity Detection",
      "Sentiment Orientation Analysis",
      "Text Sentiment Analysis"
    ]
  },
  {
    "topic_id": 6,
    "size": 55,
    "keywords": [
      "recognition facial",
      "expression recognition",
      "facial expression",
      "expression",
      "facial",
      "recognition",
      "unsupervised facial",
      "recognition masked",
      "masked",
      "masked facial"
    ],
    "examples": [
      "facial expression recognition",
      "facial expression recognition speed",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "appraisal expression recognition",
      "near-infrared facial expression recognition",
      "facial expression recognition"
    ],
    "topic_fingerprint": "9e5cc71a692e33777da95f33ee0566b980cb91d89b174cd591ddca9e38757e67",
    "l2_name": "Facial Expression Recognition",
    "definition": "The computational process of automatically identifying and classifying human emotional states from facial images or video sequences.",
    "aliases": [
      "FER",
      "facial expression analysis",
      "expression recognition",
      "facial emotion recognition",
      "unsupervised facial expression recognition",
      "masked facial expression recognition"
    ]
  },
  {
    "topic_id": 7,
    "size": 54,
    "keywords": [
      "recognition speech",
      "speech emotion",
      "speech",
      "generalization speech",
      "recognition generalization",
      "emotion recognition",
      "generalization",
      "classification speech",
      "recognition",
      "emotion"
    ],
    "examples": [
      "speech emotion classification",
      "speech emotion recognition",
      "speech emotion annotation",
      "speech emotion classification",
      "speech emotion recognition",
      "speech emotion recognition generalization",
      "speech emotion recognition",
      "speech emotion recognition"
    ],
    "topic_fingerprint": "7359a2fca3d1e58aad3508fd56ab2539a1527942167e0f54e91b6ba9040c9d03",
    "l2_name": "Speech Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states from acoustic speech signals.",
    "aliases": [
      "SER",
      "speech emotion classification",
      "emotion recognition in speech",
      "speech-based emotion detection",
      "automatic emotion recognition from speech"
    ]
  },
  {
    "topic_id": 8,
    "size": 53,
    "keywords": [
      "subtle facial",
      "subtle",
      "facial movement",
      "movement",
      "occlusion",
      "analysis subtle",
      "motion",
      "movement analysis",
      "movement detection",
      "detection subtle"
    ],
    "examples": [
      "facial feature selection",
      "handling facial occlusion",
      "handling facial occlusion",
      "subtle facial expression analysis",
      "subtle facial movement analysis",
      "subtle facial movement analysis",
      "subtle facial motion analysis",
      "intersubject facial variation suppression"
    ],
    "topic_fingerprint": "0f6f20b60d24b3bb1c8692e48a8db6dc32d4631909fdbf1a1c71d2cbbee0a75d",
    "l2_name": "Subtle Facial Movement Analysis",
    "definition": "The computational detection and analysis of minute facial motions and expressions, often in the presence of occlusion or noise.",
    "aliases": [
      "subtle facial motion analysis",
      "micro-expression analysis",
      "subtle movement detection",
      "facial motion analysis",
      "subtle facial expression analysis"
    ]
  },
  {
    "topic_id": 9,
    "size": 52,
    "keywords": [
      "task microexpression",
      "recognition task",
      "microexpression recognition",
      "task",
      "microexpression",
      "recognition",
      "challenge microexpression",
      "recognition challenge",
      "microexpression classification",
      "challenge"
    ],
    "examples": [
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task",
      "micro-expression recognition task"
    ],
    "topic_fingerprint": "c8b0d4d51aba5a37e004cc09390893dca58f8c081439735f62aa5327d7f88de7",
    "l2_name": "Micro-expression Recognition",
    "definition": "The computational task of automatically detecting and classifying fleeting, involuntary facial expressions that reveal concealed emotions.",
    "aliases": [
      "microexpression recognition",
      "micro-expression classification",
      "microexpression classification",
      "MER",
      "micro-expression detection",
      "microexpression detection",
      "recognition of micro-expressions",
      "micro-expression recognition task",
      "microexpression recognition task"
    ]
  },
  {
    "topic_id": 10,
    "size": 50,
    "keywords": [
      "subjective",
      "subjective emotion",
      "modeling subjective",
      "subjectivity",
      "subjective image",
      "perception subjectivity",
      "subjectivity handling",
      "classification subjective",
      "emotion variability",
      "emotion modeling"
    ],
    "examples": [
      "holistic perception hypothesis validation",
      "subjective emotion modeling",
      "subjective emotion variability",
      "term subjectivity classification",
      "subjective image emotion modeling",
      "subjective image emotion modeling",
      "subjective evaluation modeling",
      "subjective emotion variability modeling"
    ],
    "topic_fingerprint": "c39212f27f1edf3b803a11c4d16319fef3db47596d4c94ed16de9bf97e5fb357",
    "l2_name": "Subjective Emotion Modeling",
    "definition": "The computational modeling and analysis of human emotional responses to stimuli while accounting for inherent subjectivity, perception variability, and individual differences.",
    "aliases": [
      "subjective emotion modeling",
      "modeling subjective emotion",
      "subjectivity handling",
      "emotion variability modeling",
      "subjective image emotion modeling",
      "perception subjectivity modeling",
      "subjective evaluation modeling",
      "classification of subjective emotion",
      "holistic perception hypothesis validation"
    ]
  },
  {
    "topic_id": 11,
    "size": 49,
    "keywords": [
      "facial emotion",
      "face",
      "facial affect",
      "analysis facial",
      "facial",
      "facial identity",
      "face recognition",
      "processing",
      "facebased emotion",
      "action analysis"
    ],
    "examples": [
      "face memory recognition",
      "peripheral emotional face recognition",
      "facial emotion classification",
      "facial emotion classification",
      "facial emotion classification",
      "fearful face recognition accuracy",
      "facial identity processing",
      "facial action part localization"
    ],
    "topic_fingerprint": "e987b354485bb4dd4a9780101ea74887efd24c59919f060abdec2cbe23fd6cd7",
    "l2_name": "Facial Processing",
    "definition": "The cognitive and computational mechanisms involved in perceiving, analyzing, and interpreting facial identity, emotions, and actions.",
    "aliases": [
      "Face Processing",
      "Facial Analysis",
      "Face Recognition",
      "Facial Emotion Recognition",
      "Facial Identity Processing",
      "Face-based Emotion Analysis"
    ]
  },
  {
    "topic_id": 12,
    "size": 49,
    "keywords": [
      "action unit",
      "unit",
      "action",
      "unit detection",
      "facial action",
      "detection facial",
      "microexpression action",
      "unit recognition",
      "detection",
      "detection action"
    ],
    "examples": [
      "3d facial action unit detection",
      "facial action unit detection",
      "facial action unit recognition",
      "facial action unit detection",
      "cross-pose action unit detection",
      "cross-pose action unit detection",
      "facial action unit recognition",
      "facial action unit recognition"
    ],
    "topic_fingerprint": "64f28084bca5f11ccd16c698c5270832efc3a3352e4299260b48dac8ba13a071",
    "l2_name": "Facial Action Unit Detection",
    "definition": "The automated identification and localization of specific facial muscle movements known as Action Units, including micro-expressions and intensity estimation, from visual data.",
    "aliases": [
      "AU detection",
      "facial action unit recognition",
      "action unit detection",
      "micro-expression action unit detection",
      "3D facial action unit detection",
      "cross-pose action unit detection",
      "facial action unit intensity estimation",
      "unit detection",
      "detection facial action"
    ]
  },
  {
    "topic_id": 13,
    "size": 46,
    "keywords": [
      "video",
      "videobased",
      "affective video",
      "video emotion",
      "videos",
      "videobased emotion",
      "video based",
      "recognition video",
      "viewer emotion",
      "video classification"
    ],
    "examples": [
      "affective filtering for videos",
      "video-based emotion analysis",
      "video-based emotion analysis",
      "affective video recommendation",
      "emotion-based video classification",
      "affective video recommendation",
      "emotion-based video classification",
      "affective video classification task"
    ],
    "topic_fingerprint": "8731758a32e653ddda9694a345aa1f31c030a73ed2e26bc33f17ae88b0f816f2",
    "l2_name": "Affective Video Analysis",
    "definition": "The computational study of detecting, recognizing, and classifying emotions within video content or from viewer responses to videos.",
    "aliases": [
      "video-based emotion analysis",
      "affective video classification",
      "video emotion recognition",
      "viewer emotion analysis",
      "emotion-based video classification",
      "affective video recommendation",
      "video-based affective computing"
    ]
  },
  {
    "topic_id": 14,
    "size": 44,
    "keywords": [
      "affective",
      "affective learning",
      "modeling affective",
      "affective impact",
      "analysis affective",
      "impact affective",
      "affective dynamics",
      "effects affective",
      "identification affective",
      "affective content"
    ],
    "examples": [
      "affective learning outcomes",
      "positive affect impact",
      "affective state effects",
      "affective stroop interference",
      "affective-cognitive interdependence analysis",
      "affective response to problem solving",
      "affective content analysis",
      "affective content analysis"
    ],
    "topic_fingerprint": "27b121f2cb98c10f6085a153fb6b7d6e2cd7e7baf9550ea0da4994c354d51593",
    "l2_name": "Affective Dynamics",
    "definition": "The study of how emotional states, processes, and content influence, interact with, and are modeled within problem-solving and learning contexts.",
    "aliases": [
      "affective learning",
      "modeling affective",
      "affective impact",
      "analysis affective",
      "impact affective",
      "affective dynamics",
      "effects affective",
      "identification affective",
      "affective content",
      "affective-cognitive interdependence",
      "affective response",
      "affective priming"
    ]
  },
  {
    "topic_id": 15,
    "size": 43,
    "keywords": [
      "crossdomain",
      "crossdomain emotion",
      "recognition crossdomain",
      "crossdomain speech",
      "analysis crossdomain",
      "crossspeaker",
      "crossdomain sentiment",
      "emotion analysis",
      "speech emotion",
      "interdisciplinary"
    ],
    "examples": [
      "cross-domain emotion classification",
      "cross-domain emotion adaptation",
      "cross-domain sentiment detection",
      "domain-specific emotion detection",
      "cross-domain emotion analysis",
      "cross-domain emotion analysis",
      "cross-domain emotion recognition",
      "cross-domain emotional feature learning"
    ],
    "topic_fingerprint": "e9bd2fc64c4139e3d1eee2ee1e5a0a0f9905875822d2446274b56936bdfddcb0",
    "l2_name": "Cross-Domain Emotion Recognition",
    "definition": "The task of identifying and classifying emotional states from speech or text data when the training and testing domains differ in terms of speakers, languages, recording conditions, or contexts.",
    "aliases": [
      "cross-domain emotion classification",
      "cross-domain emotion adaptation",
      "cross-domain sentiment detection",
      "domain-specific emotion detection",
      "cross-domain emotion analysis",
      "cross-domain emotional feature learning",
      "cross-domain sentiment alignment",
      "interdisciplinary emotion analysis",
      "cross-speaker emotion recognition",
      "crossdomain emotion recognition",
      "crossdomain speech emotion analysis"
    ]
  },
  {
    "topic_id": 16,
    "size": 41,
    "keywords": [
      "mental health",
      "mental",
      "health",
      "assessment",
      "assessment mental",
      "support",
      "personalized mental",
      "health support",
      "health assessment",
      "emergency"
    ],
    "examples": [
      "psychological resilience assessment",
      "existential anxiety assessment",
      "caregiver burden assessment",
      "patient emotional distress",
      "flow experience assessment",
      "dynamic psychological assessment",
      "pandemic impact assessment",
      "risk assessment for mental patients"
    ],
    "topic_fingerprint": "efff388a61aacf0e06d8110977b8f632ea4af0c125eb0c9c8e180e658e6be3a2",
    "l2_name": "Mental Health Assessment",
    "definition": "The systematic evaluation of an individual's psychological state, emotional distress, and mental health risks to inform personalized support and intervention strategies.",
    "aliases": [
      "mental health assessment",
      "psychological assessment",
      "mental status evaluation",
      "emotional distress screening",
      "mental health risk assessment",
      "personalized mental health evaluation",
      "dynamic psychological assessment",
      "mental health monitoring"
    ]
  },
  {
    "topic_id": 17,
    "size": 40,
    "keywords": [
      "recognition multimodal",
      "multimodal emotion",
      "multimodal",
      "emotion recognition",
      "recognition",
      "emotion reasoning",
      "emotion",
      "reasoning",
      "multimodal continuous",
      "reasoning multimodal"
    ],
    "examples": [
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion recognition"
    ],
    "topic_fingerprint": "266790b0e5659253ca572936c27e8cca41ae9f1a7b7b17783b482454ef18f51a",
    "l2_name": "Multimodal Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states by integrating and analyzing data from multiple modalities such as text, audio, and visual cues.",
    "aliases": [
      "multimodal emotion recognition",
      "MER",
      "multimodal emotion reasoning",
      "emotion recognition multimodal",
      "multimodal continuous emotion recognition",
      "multimodal emotion recognition task"
    ]
  },
  {
    "topic_id": 18,
    "size": 40,
    "keywords": [
      "recognition multimodal",
      "multimodal emotion",
      "multimodal",
      "estimation multimodal",
      "emotion recognition",
      "multimodal stress",
      "stress estimation",
      "task multimodal",
      "recognition",
      "emotion"
    ],
    "examples": [
      "multimodal emotion expression",
      "multimodal emotion recognition",
      "multimodal emotion recognition",
      "multimodal emotion estimation",
      "multimodal emotion recognition task",
      "multimodal emotion inference",
      "multimodal emotion recognition",
      "continuous multimodal emotion recognition"
    ],
    "topic_fingerprint": "7c41691fc41b558a4c977790849b831e30b963e982f9e0bbdc5903a5f0edec19",
    "l2_name": "Multimodal Emotion Recognition",
    "definition": "The computational task of identifying and estimating human emotional states or stress levels by integrating and analyzing data from multiple modalities such as text, audio, and visual cues.",
    "aliases": [
      "multimodal emotion estimation",
      "multimodal stress estimation",
      "multimodal emotion inference",
      "continuous multimodal emotion recognition",
      "multimodal emotion expression analysis",
      "MER",
      "multimodal affect recognition"
    ]
  },
  {
    "topic_id": 19,
    "size": 40,
    "keywords": [
      "labeled",
      "limited",
      "labeled data",
      "data",
      "limited labeled",
      "training",
      "scarcity",
      "data scarcity",
      "data limited",
      "training data"
    ],
    "examples": [
      "utilization of unlabeled data",
      "labeled data scarcity issue",
      "labeled data scarcity issue",
      "limited annotated microblog data",
      "limited labeled data usage",
      "training testing data discrepancy",
      "limited training data",
      "supervised data scarcity"
    ],
    "topic_fingerprint": "18cafa840222566c819e6cad69dbfefaf85cb0393db43a579aa43fc6eb73aad3",
    "l2_name": "Data Scarcity",
    "definition": "The challenge of developing effective machine learning models when the availability of labeled training data is insufficient or severely limited.",
    "aliases": [
      "limited labeled data",
      "labeled data scarcity",
      "data scarcity",
      "supervised data scarcity",
      "lack of labeled data",
      "limited training data",
      "low-resource scenario",
      "limited annotated data"
    ]
  },
  {
    "topic_id": 20,
    "size": 38,
    "keywords": [
      "image emotion",
      "emotion classification",
      "image",
      "basic emotion",
      "basic",
      "classification",
      "social image",
      "classification social",
      "classification emotion",
      "emotion pattern"
    ],
    "examples": [
      "basic emotion classification",
      "emotion pattern classification",
      "image emotion recognition",
      "image emotion recognition",
      "image emotion regression",
      "image emotion regression",
      "image emotion regression",
      "automatic user emotion estimation"
    ],
    "topic_fingerprint": "cc4c7f9f0d541d00eac3c12effed1b0377313ab6559573211afd26ab2c301dbf",
    "l2_name": "Image Emotion Classification",
    "definition": "The task of automatically identifying and categorizing the emotional content or affective states evoked by images.",
    "aliases": [
      "image emotion recognition",
      "emotion classification",
      "social image emotion classification",
      "basic emotion classification",
      "emotion pattern classification",
      "automatic user emotion estimation",
      "human emotion recognition",
      "crowd emotion matching",
      "image emotion regression"
    ]
  },
  {
    "topic_id": 21,
    "size": 38,
    "keywords": [
      "mitigation domain",
      "domain",
      "shift mitigation",
      "mitigation",
      "reply",
      "shift",
      "generic",
      "generic reply",
      "domain shift",
      "reply generation"
    ],
    "examples": [
      "target-aspect pair extraction",
      "source-target corpus mismatch",
      "source-target domain mismatch",
      "distribution mismatch mitigation",
      "domain distribution mismatch",
      "generic reply reduction",
      "generic reply generation",
      "generic reply generation"
    ],
    "topic_fingerprint": "609555572c33a1279d30eb23dc9d319f61763b3a0cdc891d2fa4c50051b1805e",
    "l2_name": "Domain Shift Mitigation",
    "definition": "Techniques designed to address distribution mismatches between source and target domains to improve model generalization and reduce generic responses.",
    "aliases": [
      "domain shift",
      "source-target domain mismatch",
      "distribution mismatch mitigation",
      "domain distribution mismatch",
      "culture influence mitigation",
      "shift mitigation"
    ]
  },
  {
    "topic_id": 22,
    "size": 37,
    "keywords": [
      "interaction modeling",
      "modeling emotion",
      "dynamic emotion",
      "interaction",
      "disentanglement",
      "simulation",
      "modeling",
      "representation modeling",
      "emotional interaction",
      "dynamic"
    ],
    "examples": [
      "dynamic emotion analysis",
      "dynamic emotion analysis",
      "user-specified emotion synthesis",
      "cognition-emotion interaction modeling",
      "fine emotion modeling",
      "family emotional dynamic analysis",
      "elderly emotion interaction modeling",
      "virtual agent interaction"
    ],
    "topic_fingerprint": "84e04cf2e1b552be42ca9fefd9fa7a682e383f9118ff91cc020dd1192e1e2e49",
    "l2_name": "Dynamic Emotion Modeling",
    "definition": "The computational representation and simulation of evolving emotional states and their interactions within human-computer or multi-agent systems.",
    "aliases": [
      "dynamic emotion analysis",
      "emotion interaction modeling",
      "cognition-emotion modeling",
      "emotional dynamic simulation",
      "fine-grained emotion modeling"
    ]
  },
  {
    "topic_id": 23,
    "size": 37,
    "keywords": [
      "crossdatabase",
      "recognition crossdatabase",
      "crossdatabase microexpression",
      "microexpression recognition",
      "recognition crossdomain",
      "crossdatabase expression",
      "crossdatabase facial",
      "crossdomain facial",
      "microexpression",
      "crossdomain"
    ],
    "examples": [
      "cross-pose facial expression recognition",
      "cross-domain expression recognition",
      "cross-database facial expression recognition",
      "cross-database micro-expression recognition",
      "cross-database micro-expression recognition",
      "cross-database micro-expression recognition",
      "cross-database micro-expression recognition",
      "unsupervised cross-database expression recognition"
    ],
    "topic_fingerprint": "5074da10f5c33ea5a2a02f46c8c7c42f1532565e8c082ab71b99521751507366",
    "l2_name": "Cross-Database Expression Recognition",
    "definition": "The task of recognizing facial or micro-expressions where the training and testing data originate from different databases or domains to evaluate generalization capability.",
    "aliases": [
      "cross-database facial expression recognition",
      "cross-database micro-expression recognition",
      "cross-domain expression recognition",
      "cross-pose facial expression recognition",
      "unsupervised cross-database expression recognition",
      "cross-database performance validation",
      "crossdatabase recognition",
      "crossdomain facial recognition"
    ]
  },
  {
    "topic_id": 24,
    "size": 36,
    "keywords": [
      "dynamic facial",
      "recognition dynamic",
      "dynamic",
      "expression recognition",
      "facial expression",
      "expression",
      "facial",
      "recognition automatic",
      "automatic facial",
      "extraction dynamic"
    ],
    "examples": [
      "dynamic facial expression recognition",
      "dynamic facial expression recognition",
      "dynamic facial expression recognition",
      "dynamic facial expression recognition",
      "dynamic facial expression recognition",
      "dynamic facial expression analysis",
      "dynamic facial expression recognition",
      "dynamic facial expression recognition"
    ],
    "topic_fingerprint": "ceb4d422e1c2b6f0e79c12b78752141e5bcdae655b9975b3b22c24e5c165cb1a",
    "l2_name": "Dynamic Facial Expression Recognition",
    "definition": "The automated analysis and classification of human facial expressions using temporal sequences of video data to capture evolving emotional states.",
    "aliases": [
      "dynamic facial expression recognition",
      "automatic facial expression recognition",
      "dynamic facial expression analysis",
      "dynamic facial pattern analysis",
      "DFER",
      "video-based facial expression recognition",
      "temporal facial expression recognition"
    ]
  },
  {
    "topic_id": 25,
    "size": 36,
    "keywords": [
      "recognition facial",
      "expression recognition",
      "facial expression",
      "expression",
      "facial",
      "facial microexpression",
      "recognition",
      "expression representation",
      "recognition hierarchical",
      "hierarchical facial"
    ],
    "examples": [
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition",
      "facial expression recognition"
    ],
    "topic_fingerprint": "cee9be45ebe123f26e176bf32dcec2dfde5dec7e0172e27c65177f86ab1189ad",
    "l2_name": "Facial Expression Recognition",
    "definition": "The automated process of identifying and classifying human emotional states by analyzing facial features and movements.",
    "aliases": [
      "facial expression recognition",
      "expression recognition",
      "facial micro-expression recognition",
      "FER",
      "facial emotion recognition",
      "micro-expression recognition"
    ]
  },
  {
    "topic_id": 26,
    "size": 36,
    "keywords": [
      "recognition microexpression",
      "microexpression recognition",
      "microexpression",
      "recognition accuracy",
      "question answering",
      "visual question",
      "question",
      "microexpression visual",
      "recognition mechanisms",
      "answering"
    ],
    "examples": [
      "micro-expression recognition accuracy",
      "microexpression recognition limits",
      "micro-expression recognition",
      "microexpression recognition mechanisms",
      "macro-expression recognition",
      "micro-expression recognition",
      "micro-expression recognition",
      "macroexpression and microexpression recognition"
    ],
    "topic_fingerprint": "2aa1d7d28b15988f9c4398740e71d0e35d6a06b4eb58b25dc279a161f0679297",
    "l2_name": "Microexpression Recognition",
    "definition": "The computational or psychological process of detecting, classifying, and analyzing brief, involuntary facial expressions to determine emotional states.",
    "aliases": [
      "micro-expression recognition",
      "microexpression detection",
      "micro-expression detection",
      "facial microexpression analysis",
      "MER",
      "microexpression classification",
      "recognition of microexpressions"
    ]
  },
  {
    "topic_id": 27,
    "size": 34,
    "keywords": [
      "genuine emotion",
      "genuine",
      "detection genuine",
      "grouplevel emotion",
      "grouplevel",
      "recognition genuine",
      "emotion detection",
      "emotion concealment",
      "concealment",
      "concealment detection"
    ],
    "examples": [
      "genuine emotion recognition",
      "genuine emotion recognition",
      "genuine emotion detection",
      "genuine emotion detection",
      "genuine emotion detection",
      "genuine emotion detection",
      "genuine emotion detection",
      "genuine emotion detection"
    ],
    "topic_fingerprint": "f660916e7cda88704ff93547344cd58074e4bef8a846a42c3425453f48e2989c",
    "l2_name": "Genuine Emotion Detection",
    "definition": "The computational task of identifying and distinguishing authentic emotional expressions from concealed or fabricated ones at both individual and group levels.",
    "aliases": [
      "genuine emotion recognition",
      "detection of genuine emotion",
      "emotion concealment detection",
      "concealment detection",
      "group-level emotion recognition",
      "genuine affect detection",
      "fake emotion detection"
    ]
  },
  {
    "topic_id": 28,
    "size": 34,
    "keywords": [
      "crosscorpus",
      "recognition crosscorpus",
      "crosscorpus speech",
      "speech emotion",
      "speech",
      "crosscorpus emotion",
      "ser",
      "emotional corpus",
      "crosscorpus ser",
      "sourcefree crosscorpus"
    ],
    "examples": [
      "automatic emotional corpus construction",
      "cross-corpus emotion recognition",
      "cross-corpus speech emotion recognition",
      "cross-corpus speech emotion recognition",
      "cross-corpus speech emotion recognition",
      "cross-corpus speech emotion recognition",
      "cross-corpus speech emotion recognition",
      "cross-corpus speech emotion recognition"
    ],
    "topic_fingerprint": "f3234edd00347346bf53a4ebbb311f1be6e54d76221bbb1113dea2ac4ac6a6df",
    "l2_name": "Cross-Corpus Speech Emotion Recognition",
    "definition": "The task of training and evaluating speech emotion recognition models across different datasets to address domain mismatch and improve generalization.",
    "aliases": [
      "cross-corpus SER",
      "cross-corpus emotion recognition",
      "cross-dataset speech emotion recognition",
      "domain adaptation for SER",
      "source-free cross-corpus SER"
    ]
  },
  {
    "topic_id": 29,
    "size": 33,
    "keywords": [
      "influence modeling",
      "personality influence",
      "evaluation",
      "aesthetic",
      "perception evaluation",
      "modeling personality",
      "influence",
      "personality",
      "evaluation multimodal",
      "perception"
    ],
    "examples": [
      "aesthetic quality assessment",
      "user perception evaluation",
      "theoretical model evaluation",
      "urban color perception evaluation",
      "inter-rater agreement evaluation",
      "personal characteristic modeling",
      "multifactorial emotion perception modeling",
      "contrast effects in evaluation"
    ],
    "topic_fingerprint": "083174f88b8e7f45f46d9c754d7ab9f6a091995bfd0f81169d1fa7f5f6f92211",
    "l2_name": "Perception and Personality Modeling",
    "definition": "The computational modeling and evaluation of human perception, aesthetic judgment, and personality traits along with their mutual influences.",
    "aliases": [
      "aesthetic quality assessment",
      "user perception evaluation",
      "personality influence modeling",
      "personal characteristic modeling",
      "multifactorial emotion perception modeling",
      "personalized aesthetic assessment",
      "user preference modeling",
      "perception evaluation",
      "aesthetic perception modeling"
    ]
  },
  {
    "topic_id": 30,
    "size": 33,
    "keywords": [
      "individual",
      "individual difference",
      "difference",
      "modeling individual",
      "variation",
      "deployment",
      "application",
      "practical",
      "mitigation individual",
      "practical application"
    ],
    "examples": [
      "generalized stigma formation",
      "expression appearance variation modeling",
      "intelligent interaction design",
      "viewpoint variation effects",
      "individual difference mitigation",
      "atypical individual analysis",
      "individual intensity variation",
      "modeling individual differences"
    ],
    "topic_fingerprint": "e33fd0a27130d49c7c1a918cfff64f5d5092185c63855cd5b405b137857b10be",
    "l2_name": "Individual Difference Modeling",
    "definition": "The challenge of accounting for and mitigating variations among individuals in the modeling, deployment, and practical application of intelligent systems.",
    "aliases": [
      "individual difference mitigation",
      "modeling individual differences",
      "individual variation modeling",
      "atypical individual analysis",
      "individual difference overfitting",
      "expression appearance variation modeling",
      "viewpoint variation effects",
      "individual intensity variation"
    ]
  },
  {
    "topic_id": 31,
    "size": 32,
    "keywords": [
      "recognition spontaneous",
      "spontaneous microexpression",
      "spontaneous",
      "microexpression recognition",
      "spontaneous facial",
      "facial microexpression",
      "microexpression",
      "detection spontaneous",
      "microexpression analysis",
      "microexpression detection"
    ],
    "examples": [
      "spontaneous micro-expression recognition",
      "spontaneous micro-expression recognition",
      "spontaneous micro-expression detection",
      "spontaneous micro-expression detection",
      "spontaneous micro-expression recognition",
      "spontaneous micro-expression recognition",
      "spontaneous micro-expression recognition",
      "spontaneous micro-expression recognition"
    ],
    "topic_fingerprint": "2bd5882125939891a8d4eab686fabc0e732497ff77c697e9e274ecde6e6267ad",
    "l2_name": "Spontaneous Micro-expression Recognition",
    "definition": "The automated detection and classification of involuntary, brief facial expressions that occur naturally in uncontrolled environments.",
    "aliases": [
      "spontaneous microexpression recognition",
      "spontaneous micro-expression detection",
      "spontaneous microexpression analysis",
      "spontaneous facial microexpression recognition",
      "microexpression recognition in spontaneous settings",
      "detection of spontaneous microexpressions"
    ]
  },
  {
    "topic_id": 32,
    "size": 32,
    "keywords": [
      "continuous emotion",
      "continuous",
      "recognition continuous",
      "estimation continuous",
      "emotion dimension",
      "joint",
      "dimension",
      "intent",
      "emotion intent",
      "recognition child"
    ],
    "examples": [
      "continuous emotion dimension modeling",
      "kansei engineering application",
      "continuous emotion state modeling",
      "continuous dimensional affect estimation",
      "continuous emotion dimension estimation",
      "continuous emotion dimension estimation",
      "continuous emotion recognition",
      "continuous emotion recognition"
    ],
    "topic_fingerprint": "5181eda7b97a0c6727a54696e9b9f638bb8d96ae27a6d56d8086b086fd56c3de",
    "l2_name": "Continuous Emotion Recognition",
    "definition": "The computational task of estimating emotional states as continuous values along dimensional scales such as valence and arousal over time.",
    "aliases": [
      "continuous emotion estimation",
      "dimensional emotion recognition",
      "continuous affect estimation",
      "continuous emotion dimension modeling",
      "continuous emotion state modeling",
      "joint emotion intent recognition"
    ]
  },
  {
    "topic_id": 33,
    "size": 32,
    "keywords": [
      "crosscultural",
      "crosscultural emotion",
      "recognition crosscultural",
      "analysis crosscultural",
      "perception crosscultural",
      "crossculture",
      "crosscultural sentiment",
      "crosscultural continuous",
      "crosslanguage",
      "emotion perception"
    ],
    "examples": [
      "cross-cultural emotion measurement",
      "psychometric scale validation",
      "cross-sentence sentiment analysis",
      "cross-cultural emotion perception",
      "cross-cultural emotion perception",
      "cross-cultural emotion perception",
      "cross-culture validity assessment",
      "cross-cultural continuous emotion recognition"
    ],
    "topic_fingerprint": "99ffedd5d1e73e1b4e45fdd254c3caee9212aca592a0ab39c9aa283acc2599be",
    "l2_name": "Cross-Cultural Emotion Recognition",
    "definition": "The study and development of methods to identify, measure, and analyze human emotions across different cultural and linguistic contexts.",
    "aliases": [
      "cross-cultural emotion perception",
      "cross-cultural sentiment analysis",
      "cross-language emotion recognition",
      "cross-culture validity assessment",
      "culture-independent emotion elicitation",
      "cross-cultural affective engagement",
      "cross-cultural continuous emotion recognition"
    ]
  },
  {
    "topic_id": 34,
    "size": 32,
    "keywords": [
      "finegrained",
      "finegrained emotion",
      "classification finegrained",
      "finegrained emotional",
      "finegrained facial",
      "recognition finegrained",
      "finegrained sentiment",
      "analysis finegrained",
      "detection finegrained",
      "control finegrained"
    ],
    "examples": [
      "fine-grained sentiment annotation",
      "fine-grained emotion detection",
      "fine-grained emotion detection",
      "fine-grained emotion analysis",
      "fine-grained emotion element extraction",
      "fine-grained sentiment polarity detection",
      "fine-grained sentiment polarity",
      "fine-grained sentiment analysis"
    ],
    "topic_fingerprint": "da89ec9a8fe0f7c09f8050d0bb9b4126a4fdd5c6fb0f77e0f97d140683f0167d",
    "l2_name": "Fine-Grained Emotion Analysis",
    "definition": "The computational task of identifying, classifying, or extracting specific and nuanced emotional states, sentiments, or facial features beyond basic categories.",
    "aliases": [
      "fine-grained sentiment analysis",
      "fine-grained emotion detection",
      "fine-grained emotion classification",
      "fine-grained sentiment polarity detection",
      "fine-grained emotional tagging",
      "fine-grained facial feature capture",
      "fine-grained emotion element extraction",
      "fine-grained word classification",
      "FGEA",
      "granular emotion recognition"
    ]
  },
  {
    "topic_id": 35,
    "size": 32,
    "keywords": [
      "multimodal affective",
      "multimodal affect",
      "affect",
      "affect recognition",
      "affective analysis",
      "affect analysis",
      "multimodal",
      "analysis multimodal",
      "multiperson",
      "multiperson affect"
    ],
    "examples": [
      "multimodal affective annotation",
      "multimodal affect analysis",
      "multi-person affect analysis",
      "multi-person affect analysis",
      "multi-modal affective analysis",
      "frame-level affect recognition",
      "multimodal affect analysis",
      "multimodal affect analysis"
    ],
    "topic_fingerprint": "bd927149d9f3536540bc6cdb7726c83ee3f273cd982e16790b26b29a6f9d7f12",
    "l2_name": "Multimodal Affect Analysis",
    "definition": "The computational analysis and recognition of human emotional states by integrating and processing multiple data modalities such as audio, visual, and textual signals.",
    "aliases": [
      "multimodal affective computing",
      "multimodal emotion recognition",
      "multi-modal affect analysis",
      "affective signal integration",
      "multiperson affect analysis",
      "multimodal affective behavior analysis",
      "frame-level affect recognition",
      "affect analysis"
    ]
  },
  {
    "topic_id": 36,
    "size": 32,
    "keywords": [
      "intensity",
      "emotional reaction",
      "reaction",
      "emotional intensity",
      "prediction emotional",
      "state prediction",
      "emotional",
      "happiness",
      "control",
      "intensity estimation"
    ],
    "examples": [
      "emotional head motion prediction",
      "group emotional intensity analysis",
      "group happiness intensity estimation",
      "predictability effect on emotion",
      "emotional intensity measurement",
      "group-level happiness intensity estimation",
      "audience emotional reaction analysis",
      "emotional similarity prediction"
    ],
    "topic_fingerprint": "8b9f66ffabe62a87cae3398d4d6c673ca482969568509d4ea08bf0a957dbddab",
    "l2_name": "Emotional Intensity Estimation",
    "definition": "The computational task of predicting, measuring, or analyzing the magnitude and fluctuation of emotional states and reactions in individuals or groups.",
    "aliases": [
      "emotional intensity prediction",
      "emotion intensity measurement",
      "group emotional intensity analysis",
      "happiness intensity estimation",
      "dimensional emotional state prediction",
      "emotional reaction assessment",
      "intensity estimation",
      "affective intensity prediction"
    ]
  },
  {
    "topic_id": 37,
    "size": 31,
    "keywords": [
      "stress",
      "stress state",
      "emotional stress",
      "stress detection",
      "state recognition",
      "detection stress",
      "estimation stress",
      "emotionbased stress",
      "recognition emotional",
      "state"
    ],
    "examples": [
      "mandarin stress prediction",
      "teacher mental pressure analysis",
      "continuous speech stress detection",
      "stress corpus development",
      "emotional stress state recognition",
      "emotional stress state recognition",
      "emotional stress state recognition",
      "psychological stress detection"
    ],
    "topic_fingerprint": "6856e64b5ca623eca124d4aa97adac68209a55c6c48cd4f18a58c7076e4d4745",
    "l2_name": "Stress Detection and Recognition",
    "definition": "The computational identification, estimation, and classification of human psychological or emotional stress states using various data modalities.",
    "aliases": [
      "stress state recognition",
      "emotional stress detection",
      "stress estimation",
      "psychological stress analysis",
      "stress state estimation",
      "emotion-based stress detection"
    ]
  },
  {
    "topic_id": 38,
    "size": 30,
    "keywords": [
      "music",
      "music emotion",
      "affective music",
      "image musicalization",
      "emotionbased music",
      "classification music",
      "emotional design",
      "audio emotion",
      "music recommendation",
      "musicalization"
    ],
    "examples": [
      "audio emotion recognition",
      "image musicalization",
      "emotion-based music selection",
      "image musicalization task",
      "affective music classification",
      "web music emotion recognition",
      "dynamic texture description",
      "fuzzy music search"
    ],
    "topic_fingerprint": "ed4655fe572340d9aacf7ee96e2a9a21bf9b15b5f5a3849ec7fbff595978136d",
    "l2_name": "Music Emotion Analysis",
    "definition": "The computational study of recognizing, classifying, and generating music based on its emotional content and affective properties.",
    "aliases": [
      "Affective Music Computing",
      "Music Emotion Recognition",
      "Emotion-based Music Classification",
      "Music Mood Analysis",
      "Audio Emotion Detection",
      "Musicalization",
      "Emotional Music Recommendation"
    ]
  },
  {
    "topic_id": 39,
    "size": 30,
    "keywords": [
      "wild",
      "inthewild",
      "recognition wild",
      "recognition inthewild",
      "wild emotion",
      "inthewild affect",
      "wild environment",
      "inthewild facial",
      "environment",
      "analysis inthewild"
    ],
    "examples": [
      "emotion recognition in wild",
      "emotion recognition in the wild",
      "expression recognition in wild",
      "emotion recognition in wild",
      "emotion recognition in the wild",
      "emotion recognition in wild",
      "in-the-wild facial behavior analysis",
      "wild environment classification"
    ],
    "topic_fingerprint": "bb0efd11d8052d3654f1b8117926099ac1b72caef9490d65a82cc29b8ad280bd",
    "l2_name": "In-the-Wild Emotion Recognition",
    "definition": "The task of detecting and classifying human emotions or affective states from facial expressions and behaviors captured in uncontrolled, real-world environments.",
    "aliases": [
      "emotion recognition in the wild",
      "in-the-wild affect recognition",
      "wild emotion analysis",
      "in-the-wild facial behavior analysis",
      "expression recognition in the wild",
      "in-the-wild affect prediction",
      "wild environment emotion detection"
    ]
  },
  {
    "topic_id": 40,
    "size": 30,
    "keywords": [
      "expression analysis",
      "analysis facial",
      "facial expression",
      "expression",
      "involuntary facial",
      "involuntary",
      "analysis involuntary",
      "facial",
      "effect facial",
      "expression confusion"
    ],
    "examples": [
      "angry expression effect",
      "facial expression analysis",
      "involuntary facial expression analysis",
      "involuntary facial expression analysis",
      "involuntary facial expression analysis",
      "gaze and expression interaction",
      "facial expression decoding",
      "facial expression processing"
    ],
    "topic_fingerprint": "4b276db36ce5bc4fb1e299c6b02c29507a44131d31d522bc0bb75bba9442eb73",
    "l2_name": "Facial Expression Analysis",
    "definition": "The computational or psychological study of detecting, interpreting, and decoding human facial movements, including involuntary reactions and emotional states.",
    "aliases": [
      "facial expression analysis",
      "analysis facial",
      "expression analysis",
      "involuntary facial expression analysis",
      "facial expression decoding",
      "facial expression processing",
      "effect facial",
      "angry expression effect",
      "gaze and expression interaction",
      "facial expression self-awareness",
      "expression confusion"
    ]
  },
  {
    "topic_id": 41,
    "size": 29,
    "keywords": [
      "severity",
      "depression severity",
      "depression",
      "severity estimation",
      "assessment depression",
      "estimation depression",
      "prediction depression",
      "symptom",
      "depression assessment",
      "correlation depression"
    ],
    "examples": [
      "depression and physical comorbidity",
      "quality of life in depression",
      "depression severity estimation",
      "depression severity measurement",
      "depression scale prediction",
      "depression severity correlation",
      "depression severity assessment",
      "depression severity prediction"
    ],
    "topic_fingerprint": "7a92a2d3662cecd4547fd2030a4af97a4a925bce994e36ab85bb2800285a2d6d",
    "l2_name": "Depression Severity Assessment",
    "definition": "The evaluation, measurement, and prediction of the intensity of depressive symptoms and their correlation with clinical outcomes.",
    "aliases": [
      "depression severity estimation",
      "depression severity measurement",
      "depression severity prediction",
      "assessment of depression severity",
      "estimation of depression severity",
      "prediction of depression severity",
      "depression scale prediction",
      "clinical symptom severity correlation",
      "severity of depression"
    ]
  },
  {
    "topic_id": 42,
    "size": 29,
    "keywords": [
      "depression detection",
      "depression",
      "detection depression",
      "depression recognition",
      "task depression",
      "detection",
      "detection task",
      "depressive state",
      "eegbased depression",
      "depression state"
    ],
    "examples": [
      "depression self-abnormality detection",
      "depression detection task",
      "real-time clinical depression detection",
      "depression state diagnosis",
      "depression recognition from eeg",
      "explainable depression detection",
      "depressive state extraction",
      "depressive state extraction"
    ],
    "topic_fingerprint": "f855598c37d2e28961ff94e62e7b376b881bdb9eee18e95ef2c391bf717f6f69",
    "l2_name": "Depression Detection",
    "definition": "The computational task of identifying depressive states or disorders using data modalities such as EEG, interviews, or behavioral signals.",
    "aliases": [
      "depression recognition",
      "depressive state detection",
      "depression diagnosis",
      "EEG-based depression detection",
      "depression state extraction",
      "clinical depression detection"
    ]
  },
  {
    "topic_id": 43,
    "size": 29,
    "keywords": [
      "affective pattern",
      "affective image",
      "image classification",
      "affective region",
      "classification affective",
      "affective",
      "image",
      "pattern",
      "pattern recognition",
      "mapping"
    ],
    "examples": [
      "affective trait mapping",
      "affective pattern recognition",
      "hci-related affect recognition",
      "affective basis of extraversion",
      "affective trait mapping",
      "affective image classification",
      "affective image classification",
      "affective image classification"
    ],
    "topic_fingerprint": "0ee05a0876890f18c46d50486dd5b83a81feb63706020d11c5ccc400c299e240",
    "l2_name": "Affective Image Analysis",
    "definition": "The computational problem of identifying, classifying, and mapping emotional patterns and regions within visual data.",
    "aliases": [
      "affective pattern recognition",
      "affective image classification",
      "affective region discovery",
      "affective trait mapping",
      "emotion recognition in images",
      "visual affect analysis"
    ]
  },
  {
    "topic_id": 44,
    "size": 28,
    "keywords": [
      "tendency",
      "emotion annotation",
      "descriptive",
      "descriptive emotion",
      "emotion label",
      "annotation",
      "expensive",
      "ignored subtle",
      "expensive emotion",
      "emotional tendency"
    ],
    "examples": [
      "emotional tendency judgment",
      "anger expression processing",
      "interoperable emotion annotation definition",
      "emotion tendency labeling",
      "emotion tendency judgment",
      "emotional tendency mining",
      "emotional tendency judgment",
      "unified emotion annotation criteria"
    ],
    "topic_fingerprint": "65f18a7461ce8d85f840da27fd09051764de5750b5d46489706c822903185963",
    "l2_name": "Emotional Tendency Annotation",
    "definition": "The process of identifying, labeling, and standardizing the directional polarity or subtle nuances of emotions within data, often addressing challenges related to cost and consistency.",
    "aliases": [
      "emotion tendency labeling",
      "emotional tendency judgment",
      "emotion annotation",
      "descriptive emotion labeling",
      "subtle emotion detection",
      "unified emotion annotation",
      "emotion tendency mining"
    ]
  },
  {
    "topic_id": 45,
    "size": 27,
    "keywords": [
      "audiovisual",
      "recognition audiovisual",
      "audiovisual emotion",
      "emotional content",
      "openvocabulary",
      "content",
      "content generation",
      "recognition audiovideo",
      "audiovideo",
      "interactive emotional"
    ],
    "examples": [
      "audio video fusion",
      "emotional audio-visual generation",
      "audio-visual emotion recognition",
      "audio-visual affect recognition",
      "audio-visual synchronization",
      "audio-visual emotion recognition",
      "audio-video emotion recognition",
      "audio-only emotion recognition"
    ],
    "topic_fingerprint": "bb9c7cb8e3a983667bd7610896198ea908a7921a848b1342b11e8b195ca574aa",
    "l2_name": "Audio-Visual Emotion Recognition",
    "definition": "The computational task of identifying and classifying emotional states by analyzing and fusing information from both audio and visual modalities.",
    "aliases": [
      "audiovisual emotion recognition",
      "audio-video emotion recognition",
      "audio-visual affect recognition",
      "multimodal emotion recognition",
      "emotional content recognition",
      "audiovideo emotion analysis",
      "open-vocabulary audiovisual emotion recognition"
    ]
  },
  {
    "topic_id": 46,
    "size": 26,
    "keywords": [
      "multilabel",
      "multiclass",
      "multilabel emotion",
      "multiclass emotion",
      "multiemotion",
      "intensity prediction",
      "prediction multilabel",
      "multilabel affective",
      "emotion intensity",
      "classification multilabel"
    ],
    "examples": [
      "multi-emotion state labeling",
      "multi-class emotion classification",
      "multi-emotion labeling analysis",
      "multi-class speech emotion recognition",
      "multi-class emotion classification",
      "multi-label affective classification",
      "multi-emotion image labeling",
      "multi-class emotion confusion"
    ],
    "topic_fingerprint": "ee35c9f3e0ea109e5d2fcd35f149b5f6c7653ddf91127656092b3fe07ed60b49",
    "l2_name": "Multi-label and Multi-class Emotion Classification",
    "definition": "This problem involves assigning one or multiple emotion categories to data instances, potentially including the prediction of emotion intensity levels.",
    "aliases": [
      "multilabel emotion classification",
      "multiclass emotion classification",
      "multi-emotion labeling",
      "multi-emotion state labeling",
      "emotion intensity prediction",
      "multilabel affective classification",
      "multi-class speech emotion recognition",
      "multi-emotion image labeling",
      "three-category emotion recognition"
    ]
  },
  {
    "topic_id": 47,
    "size": 26,
    "keywords": [
      "classification multimodal",
      "multimodal emotion",
      "interpretation multimodal",
      "prediction multimodal",
      "emotion interpretation",
      "interpretation",
      "multimodal",
      "emotion understanding",
      "understanding multimodal",
      "understanding"
    ],
    "examples": [
      "multimodal emotion prediction",
      "multimodal emotion classification",
      "multimodal emotion modeling",
      "multimodal emotion state classification",
      "multimodal emotion classification",
      "multimodal emotion classification",
      "multimodal emotion prediction",
      "multimodal emotion prediction"
    ],
    "topic_fingerprint": "4cf8a0d5f2c8ea01cf466ca3c772a4470a3d180e040c2b65ecd7e195e4815f65",
    "l2_name": "Multimodal Emotion Classification",
    "definition": "The task of identifying and categorizing human emotional states by integrating and analyzing data from multiple modalities such as text, audio, and visual cues.",
    "aliases": [
      "multimodal emotion prediction",
      "multimodal emotion modeling",
      "multimodal emotion state classification",
      "multimodal emotion understanding",
      "multimodal emotion indexing",
      "multi dimensional emotion classification",
      "classification multimodal",
      "multimodal emotion",
      "interpretation multimodal",
      "prediction multimodal",
      "emotion interpretation",
      "emotion understanding",
      "understanding multimodal"
    ]
  },
  {
    "topic_id": 48,
    "size": 26,
    "keywords": [
      "expression classification",
      "classification facial",
      "facial expression",
      "expression detection",
      "expression",
      "coding",
      "ensemble",
      "ensemble coding",
      "alien",
      "alien expression"
    ],
    "examples": [
      "facial expression fidelity improvement",
      "facial expression feature learning",
      "facial expression identification",
      "facial expression classification",
      "facial expression classification",
      "facial expression classification",
      "facial expression detection",
      "neural coding strategies"
    ],
    "topic_fingerprint": "669b4483072ecedd23bf331a0ddb6313697d4b48b21bec611af0179c64b85727",
    "l2_name": "Facial Expression Analysis",
    "definition": "The computational study of detecting, classifying, and coding human facial expressions to interpret emotional states.",
    "aliases": [
      "facial expression classification",
      "facial expression detection",
      "expression recognition",
      "facial emotion analysis",
      "expression coding",
      "ensemble coding of facial expressions",
      "facial expression identification",
      "facial expression categorization"
    ]
  },
  {
    "topic_id": 49,
    "size": 26,
    "keywords": [
      "task speech",
      "recognition task",
      "speech emotion",
      "speech",
      "task",
      "emotion recognition",
      "tasks speech",
      "recognition",
      "emotion",
      "recognition tasks"
    ],
    "examples": [
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task",
      "speech emotion recognition task"
    ],
    "topic_fingerprint": "56e1927e9c46a35a4dba3de3c975df72ea3f70e1577507cc7ae997072a5988a8",
    "l2_name": "Speech Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states from speech signals.",
    "aliases": [
      "SER",
      "speech emotion recognition task",
      "emotion recognition in speech",
      "speech-based emotion recognition",
      "automatic emotion recognition from speech",
      "emotion detection in speech"
    ]
  },
  {
    "topic_id": 50,
    "size": 25,
    "keywords": [
      "physiological",
      "physiological signal",
      "signal",
      "multimodal physiological",
      "recognition physiological",
      "physiologicalemotion",
      "physiologicalemotion recognition",
      "signal analysis",
      "biomarker",
      "recognition physiologicalemotion"
    ],
    "examples": [
      "sentiment classification from physiology",
      "emotion recognition from physiological signals",
      "psychophysiological emotion modeling",
      "sentiment recognition from physiological signals",
      "multimodal physiological signal analysis",
      "multimodal physiological signal correlation",
      "physiological signal based recognition",
      "physiological-emotion recognition"
    ],
    "topic_fingerprint": "60075c4c3b60daebcbea2fde7ef340284bf8e67782701fbcb644437f8c5f016b",
    "l2_name": "Physiological Emotion Recognition",
    "definition": "The computational analysis of physiological signals to detect, classify, and model human emotional states.",
    "aliases": [
      "physiological signal based recognition",
      "psychophysiological emotion modeling",
      "multimodal physiological signal analysis",
      "sentiment classification from physiology",
      "physiological-emotion recognition",
      "biomarker-based emotion detection",
      "physiological signal monitoring"
    ]
  },
  {
    "topic_id": 51,
    "size": 25,
    "keywords": [
      "conversation generation",
      "conversation",
      "generation emotional",
      "emotional conversation",
      "generation",
      "guidance",
      "emotional support",
      "dialogue generation",
      "emotional guidance",
      "dialogue"
    ],
    "examples": [
      "emotional conversation generation",
      "emotional conversation generation",
      "emotional conversation generation",
      "emotional conversation generation",
      "emotional conversation generation",
      "emotion-constrained dialogue generation",
      "emotion-constrained conversation generation",
      "emotional editing in dialogue"
    ],
    "topic_fingerprint": "13fffec564bd168b87850f2311f3e620229e397e97dcdd1d812e7208d74178c5",
    "l2_name": "Emotional Conversation Generation",
    "definition": "The task of generating dialogue responses that incorporate specific emotional attributes or provide emotional support and guidance.",
    "aliases": [
      "emotional dialogue generation",
      "emotion-constrained conversation generation",
      "emotion-constrained dialogue generation",
      "emotional guidance generation",
      "emotional editing in dialogue",
      "affective conversation generation",
      "empathetic dialogue generation"
    ]
  },
  {
    "topic_id": 52,
    "size": 25,
    "keywords": [
      "arousal estimation",
      "arousal",
      "estimation",
      "valence",
      "valence arousal",
      "estimation valence",
      "valencearousal estimation",
      "valencearousal",
      "valence estimation",
      "estimation continuous"
    ],
    "examples": [
      "dimensional valence-arousal estimation",
      "dimensional valence-arousal estimation",
      "dimensional valence-arousal estimation",
      "in-the-wild arousal estimation",
      "in-the-wild valence estimation",
      "valence and arousal estimation",
      "arousal and valence estimation",
      "continuous valence arousal estimation"
    ],
    "topic_fingerprint": "03e7a3f5c8179b28745e3012036d945823a3be98d1e273ebec48027103ae7f39",
    "l2_name": "Valence-Arousal Estimation",
    "definition": "The computational task of predicting continuous dimensional values for valence and arousal to quantify emotional states.",
    "aliases": [
      "arousal estimation",
      "valence estimation",
      "dimensional valence-arousal estimation",
      "continuous valence arousal estimation",
      "valence and arousal estimation",
      "arousal and valence estimation",
      "VA estimation",
      "in-the-wild arousal estimation",
      "in-the-wild valence estimation"
    ]
  },
  {
    "topic_id": 53,
    "size": 24,
    "keywords": [
      "feature distribution",
      "distribution",
      "feature",
      "distribution mismatch",
      "inconsistency feature",
      "distribution inconsistency",
      "mismatch",
      "mismatch feature",
      "inconsistency",
      "distribution discrepancy"
    ],
    "examples": [
      "feature distribution discrepancy mitigation",
      "feature distribution mismatch",
      "feature distribution mismatch",
      "feature distribution mismatch",
      "feature distribution mismatch",
      "feature matching problem",
      "feature distribution inconsistency",
      "feature distribution inconsistency"
    ],
    "topic_fingerprint": "90fc51a82e11853df01ad2a109802d8300af8ba07aff89c3ca691e5081da6884",
    "l2_name": "Feature Distribution Mismatch",
    "definition": "The problem arising from inconsistencies or discrepancies in the statistical distribution of features between different datasets or domains.",
    "aliases": [
      "feature distribution discrepancy",
      "distribution mismatch",
      "feature distribution inconsistency",
      "distribution discrepancy",
      "feature distribution gap",
      "mismatch feature",
      "inconsistency feature",
      "feature matching problem"
    ]
  },
  {
    "topic_id": 54,
    "size": 24,
    "keywords": [
      "state classification",
      "classification affective",
      "affective state",
      "affective signal",
      "state",
      "affective",
      "classification",
      "signal classification",
      "signal",
      "signal generalization"
    ],
    "examples": [
      "affective state assessment",
      "naturalistic affective expression classification",
      "affective signal extraction from microblogs",
      "affective state classification",
      "affective state classification from signals",
      "affective state classification",
      "affective speaker state classification",
      "health state classification"
    ],
    "topic_fingerprint": "e91b7f5cc9214b1de44c746c4bd33f6ffaa21c6d300864e01d9fa73dbbb4c139",
    "l2_name": "Affective State Classification",
    "definition": "The computational task of identifying and categorizing human emotional states from various signals such as speech, text, or physiological data.",
    "aliases": [
      "affective state assessment",
      "emotion classification",
      "affective signal classification",
      "state classification",
      "affective speaker state classification",
      "real-time affective state classification"
    ]
  },
  {
    "topic_id": 55,
    "size": 24,
    "keywords": [
      "eeg emotion",
      "recognition eeg",
      "eeg",
      "interpretability eeg",
      "interpretability",
      "recognition interpretability",
      "emotion recognition",
      "coarsegrained eeg",
      "overfitting eeg",
      "eeg interpretability"
    ],
    "examples": [
      "eeg emotion recognition",
      "eeg emotion recognition",
      "eeg emotion recognition",
      "eeg emotion recognition",
      "eeg emotion recognition",
      "eeg emotion recognition",
      "eeg emotion recognition",
      "emotion recognition from eeg"
    ],
    "topic_fingerprint": "3fc85a76bdab6e0baf9d4fa67ed29050562bac1fc7725b777c9baa783b261228",
    "l2_name": "EEG Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states from electroencephalogram signals while addressing challenges such as model interpretability, overfitting, and feature granularity.",
    "aliases": [
      "emotion recognition from eeg",
      "eeg-based emotion recognition",
      "eeg emotion classification",
      "interpretable eeg emotion recognition",
      "coarse-grained eeg emotion recognition"
    ]
  },
  {
    "topic_id": 56,
    "size": 24,
    "keywords": [
      "eeg",
      "multichannel",
      "multichannel eeg",
      "recognition multichannel",
      "differences eeg",
      "eeg channel",
      "eeg signal",
      "channel",
      "signal",
      "individual differences"
    ],
    "examples": [
      "automatic eeg channel selection",
      "multichannel eeg emotion recognition",
      "dynamic eeg region relationships",
      "individual differences in eeg",
      "eeg feature extraction difficulty",
      "multi-channel eeg emotion recognition",
      "multi-channel eeg emotion recognition",
      "multi-channel eeg emotion recognition"
    ],
    "topic_fingerprint": "4f3c43743ade8e5f88b1fe009e8d6d06452f7a56efac43a67d793d052092915b",
    "l2_name": "Multichannel EEG Analysis",
    "definition": "The computational processing and interpretation of electroencephalography signals recorded from multiple electrodes to address challenges in feature extraction, channel selection, and individual variability.",
    "aliases": [
      "multichannel eeg",
      "multi-channel eeg",
      "eeg channel analysis",
      "eeg signal processing",
      "multichannel eeg recognition",
      "eeg topology analysis"
    ]
  },
  {
    "topic_id": 57,
    "size": 24,
    "keywords": [
      "crossmodal",
      "crossmodal emotion",
      "crossmodal interaction",
      "crossmodal semantic",
      "alignment",
      "semantic",
      "inefficiency crossmodal",
      "crossmodal feature",
      "interaction inefficiency",
      "semantic misalignment"
    ],
    "examples": [
      "multimodal temporal alignment",
      "sentiment-oriented cross-modal retrieval",
      "cross-modal emotion recognition",
      "cross-modal emotion matching",
      "cross-modal emotion matching",
      "multimodal interaction limitations",
      "cross-modal interaction modeling",
      "cross-modal emotion analysis"
    ],
    "topic_fingerprint": "220185e7fa6ca9f2a73bc10c02ac24fadacdacde6e9d814246a47717bd213bc3",
    "l2_name": "Cross-Modal Interaction Inefficiency",
    "definition": "The problem of suboptimal alignment and ineffective feature fusion between different modalities that hinders accurate semantic understanding and emotion recognition.",
    "aliases": [
      "crossmodal interaction inefficiency",
      "semantic misalignment",
      "inefficient cross-modal interaction",
      "cross-modal feature misalignment",
      "multimodal interaction limitations",
      "crossmodal semantic gap"
    ]
  },
  {
    "topic_id": 58,
    "size": 23,
    "keywords": [
      "personality",
      "personality recognition",
      "emotion personality",
      "recognition personality",
      "recognition emotion",
      "personality trait",
      "personalityaware",
      "workplace",
      "recognition personalityaware",
      "analysis workplace"
    ],
    "examples": [
      "personal emotional factor integration",
      "positive versus negative emotion recognition",
      "personality trait detection",
      "personnel psychology analysis",
      "workplace emotion dynamics",
      "social approval perception modeling",
      "personality detection in interactions",
      "personality recognition"
    ],
    "topic_fingerprint": "7e2ede4a06cd8559be67b77bbe8b9fb380ea3037ed49819cad1e7683240996b0",
    "l2_name": "Personality Recognition",
    "definition": "The computational detection and analysis of personality traits and their interplay with emotions, particularly within workplace and social interaction contexts.",
    "aliases": [
      "personality trait detection",
      "personality-aware emotion recognition",
      "personality analysis",
      "workplace personality modeling",
      "emotion-personality integration"
    ]
  },
  {
    "topic_id": 59,
    "size": 23,
    "keywords": [
      "social media",
      "media",
      "chinese",
      "microblog",
      "analysis social",
      "microblog sentiment",
      "media sentiment",
      "analysis chinese",
      "social",
      "media emotion"
    ],
    "examples": [
      "chinese emotion expression",
      "micro-blog sentiment classification",
      "microblog sentiment prediction",
      "fragmented chinese text emotion",
      "microblogging hot event sentiment",
      "chinese microblog emotion analysis",
      "chinese micro-blog sentiment analysis",
      "chinese sentiment lexicon construction"
    ],
    "topic_fingerprint": "01ce3b89b39d56f4f7bc932346737603508b01b0fa32db3d62027fe171a5f7f5",
    "l2_name": "Social Media Sentiment Analysis",
    "definition": "The computational process of identifying, extracting, and classifying emotional expressions and opinions from social media content, with a specific focus on Chinese microblogs.",
    "aliases": [
      "Microblog sentiment analysis",
      "Chinese microblog emotion analysis",
      "Social media emotion detection",
      "Micro-blog sentiment classification",
      "Chinese text emotion detection",
      "Microblogging hot event sentiment",
      "Fragmented Chinese text emotion analysis",
      "Chinese sentiment lexicon construction",
      "Media sentiment analysis",
      "Microblog emotion prediction"
    ]
  },
  {
    "topic_id": 60,
    "size": 23,
    "keywords": [
      "response",
      "response generation",
      "empathy",
      "emotional understanding",
      "empathetic response",
      "empathetic",
      "understanding response",
      "generation",
      "emotional response",
      "response emotional"
    ],
    "examples": [
      "response competition processes",
      "emotional response generation",
      "customizable emotional response",
      "emotionless response generation",
      "children empathy ability analysis",
      "affective response generation",
      "response quality preservation",
      "retail service empathy"
    ],
    "topic_fingerprint": "5c35b4711da3e4f5d94e80fb894ab8553d0fa34887e5d79842ef0a15bf89ec40",
    "l2_name": "Empathetic Response Generation",
    "definition": "The computational process of generating responses that demonstrate emotional understanding and empathy towards the user.",
    "aliases": [
      "empathetic response",
      "emotional response generation",
      "affective response generation",
      "emotion-aware response",
      "empathy in dialogue"
    ]
  },
  {
    "topic_id": 61,
    "size": 23,
    "keywords": [
      "arousal",
      "valence",
      "arousal prediction",
      "arousal valence",
      "valence arousal",
      "valence prediction",
      "prediction arousal",
      "prediction valence",
      "dominance",
      "prediction"
    ],
    "examples": [
      "valence and arousal classification",
      "social dominance evaluation",
      "valence and arousal prediction",
      "valence arousal dominance classification",
      "interaction of attention arousal valence",
      "valence and arousal classification",
      "arousal valence dominance prediction",
      "arousal level mismatch analysis"
    ],
    "topic_fingerprint": "6961e26bb2b7aa6bb28e4f7310592c5274a5bc12273eab5d6f76a827e6095bb9",
    "l2_name": "Affective State Prediction",
    "definition": "The computational task of estimating or classifying emotional dimensions such as valence, arousal, and dominance from data.",
    "aliases": [
      "valence arousal prediction",
      "arousal valence classification",
      "VAD prediction",
      "emotion dimension estimation",
      "affective computing prediction",
      "valence and arousal modeling"
    ]
  },
  {
    "topic_id": 62,
    "size": 22,
    "keywords": [
      "conversational emotion",
      "conversational",
      "recognition conversational",
      "analysis conversational",
      "emotion correction",
      "detection conversational",
      "correction conversational",
      "dialogical",
      "dialogical emotion",
      "task conversational"
    ],
    "examples": [
      "natural speech emotion analysis",
      "affective dialogue consistency",
      "emotion-based robot communication",
      "conversation anomaly detection",
      "conversational emotion analysis",
      "conversational emotion analysis",
      "conversational emotion recognition task",
      "conversational emotion recognition"
    ],
    "topic_fingerprint": "a88445e8ed3a6871959f9916e00deb9cda88c93a9754d340ebbfea3ed0bbddfa",
    "l2_name": "Conversational Emotion Recognition",
    "definition": "The computational task of detecting, analyzing, and correcting emotional states within multi-turn dialogues or conversational contexts.",
    "aliases": [
      "conversational emotion analysis",
      "dialogical emotion recognition",
      "emotion detection in conversation",
      "conversational affect recognition",
      "dialogue emotion correction",
      "conversational emotion recognition task",
      "affective dialogue analysis"
    ]
  },
  {
    "topic_id": 63,
    "size": 22,
    "keywords": [
      "prediction continuous",
      "continuous emotion",
      "continuous",
      "emotion prediction",
      "prediction",
      "distribution prediction",
      "emotion distribution",
      "distribution",
      "friendliness",
      "emotionenhanced performance"
    ],
    "examples": [
      "continuous affect prediction",
      "continuous emotion prediction",
      "continuous emotion prediction",
      "continuous emotion distribution prediction",
      "continuous emotion distribution prediction",
      "friendliness degree prediction",
      "continuous emotion distribution prediction",
      "continuous emotion distribution prediction"
    ],
    "topic_fingerprint": "582688fd026c650211995437099367e8399eb3e85bdf722c48de3c2050d52ea5",
    "l2_name": "Continuous Emotion Prediction",
    "definition": "The task of predicting emotional states as continuous values or distributions over time rather than discrete categories.",
    "aliases": [
      "continuous affect prediction",
      "continuous emotion distribution prediction",
      "emotion distribution prediction",
      "friendliness degree prediction",
      "continuous emotion recognition",
      "dimensional emotion prediction"
    ]
  },
  {
    "topic_id": 64,
    "size": 21,
    "keywords": [
      "spotting spontaneous",
      "spontaneous",
      "spotting",
      "microexpression spotting",
      "spotting recognition",
      "spontaneous microexpression",
      "spontaneous expression",
      "spontaneous facial",
      "differentiation spontaneous",
      "integrated spotting"
    ],
    "examples": [
      "spontaneous micro-expression spotting",
      "spontaneous micro-expression spotting",
      "spontaneous versus posed differentiation",
      "spontaneous versus posed expression differentiation",
      "spontaneous micro-expression spotting",
      "spontaneous pain expression monitoring",
      "spontaneous expression recognition",
      "spontaneous facial behavior analysis"
    ],
    "topic_fingerprint": "8bb2b843aedbc681b44c088f3048050c790b1aea74ec09174d9c3fdcb5767d64",
    "l2_name": "Spontaneous Micro-expression Spotting",
    "definition": "The automated detection and temporal localization of brief, involuntary facial expressions that occur naturally in uncontrolled environments.",
    "aliases": [
      "spontaneous micro-expression spotting",
      "microexpression spotting",
      "spotting spontaneous",
      "spontaneous expression spotting",
      "spontaneous facial behavior analysis",
      "spontaneous versus posed differentiation",
      "integrated spotting"
    ]
  },
  {
    "topic_id": 65,
    "size": 21,
    "keywords": [
      "affective state",
      "state",
      "recognition affective",
      "state recognition",
      "monitoring affective",
      "detection affective",
      "affective",
      "state detection",
      "affect detection",
      "viewer affective"
    ],
    "examples": [
      "user affect measurement",
      "viewer affective state estimation",
      "viewer affective state analysis",
      "suppressed affect detection",
      "suppressed affect detection",
      "user affective state tracking",
      "affective state recognition",
      "affective state recognition"
    ],
    "topic_fingerprint": "722edf5ef60bd632ae0362603d9c2a6c402f8cba2b672f6016d4805116ac0dcb",
    "l2_name": "Affective State Recognition",
    "definition": "The computational process of detecting, monitoring, and analyzing human emotional states through behavioral and physiological signals.",
    "aliases": [
      "affect detection",
      "emotion recognition",
      "affective state estimation",
      "user affect measurement",
      "viewer affective state analysis",
      "suppressed affect detection",
      "affective behavior detection",
      "state recognition",
      "affect monitoring"
    ]
  },
  {
    "topic_id": 66,
    "size": 21,
    "keywords": [
      "domain adaptation",
      "adaptation",
      "domain",
      "unsupervised domain",
      "unsupervised",
      "adaptation unsupervised",
      "adaptation emotion",
      "adaptation mismatch",
      "adaptation multisource",
      "shift unsupervised"
    ],
    "examples": [
      "domain adaptation for emotion",
      "domain adaptation for emotion",
      "cross subject domain shift",
      "unsupervised domain adaptation",
      "unsupervised domain adaptation",
      "unsupervised domain adaptation",
      "unsupervised domain adaptation",
      "unsupervised domain adaptation"
    ],
    "topic_fingerprint": "92c228d7ad13b2a4fea66f96f5fef01289d56ea4635a0343fb55e5bba0527662",
    "l2_name": "Unsupervised Domain Adaptation",
    "definition": "The problem of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain despite distribution shifts.",
    "aliases": [
      "UDA",
      "unsupervised adaptation",
      "domain shift",
      "cross-domain adaptation",
      "multi-source domain adaptation",
      "domain mismatch"
    ]
  },
  {
    "topic_id": 67,
    "size": 20,
    "keywords": [
      "brain",
      "connectivity",
      "brain connectivity",
      "functional",
      "brain dynamics",
      "connectivity modeling",
      "connectivity analysis",
      "brain functional",
      "analysis brain",
      "connectivity pattern"
    ],
    "examples": [
      "grey matter volume alterations",
      "inconsistent structural brain findings",
      "brain hemisphere discrepancy modeling",
      "modeling localized brain functional relations",
      "brain region abstraction",
      "functional connection modeling",
      "abnormal brain information interaction",
      "brain connectivity analysis"
    ],
    "topic_fingerprint": "bf2b5a9836c0d683ec0e1e610a4c2051a54aff7efd3c6afe5e6bd13b8b5f4bfe",
    "l2_name": "Brain Connectivity Analysis",
    "definition": "The computational and statistical modeling of functional and structural interactions between brain regions to characterize neural dynamics and connectivity patterns.",
    "aliases": [
      "functional connectivity",
      "brain connectivity",
      "connectivity modeling",
      "connectivity analysis",
      "brain dynamics",
      "functional connection modeling",
      "abnormal brain information interaction",
      "brain region abstraction"
    ]
  },
  {
    "topic_id": 68,
    "size": 20,
    "keywords": [
      "autism",
      "autism emotion",
      "spectrum",
      "recognition autism",
      "autism spectrum",
      "spectrum disorder",
      "assessment autism",
      "autistic",
      "asd autism",
      "social skill"
    ],
    "examples": [
      "autism neural substrate identification",
      "autistic behavior assessment",
      "autism therapy engagement recognition",
      "autism spectrum disorder therapy",
      "social skill improvement for asd",
      "autism severity assessment",
      "early autism symptom prediction",
      "attention deficit assessment"
    ],
    "topic_fingerprint": "b4393320900b48fce56a8e435fd9d60654e873f3f26abecb48a5cb27c51b56ec",
    "l2_name": "Autism Spectrum Disorder Assessment",
    "definition": "The evaluation, screening, and analysis of autism spectrum disorder including symptom severity, emotional recognition, social skills, and neural substrates.",
    "aliases": [
      "ASD assessment",
      "autism screening",
      "autistic behavior evaluation",
      "autism severity measurement",
      "early autism detection",
      "ASD diagnosis",
      "autism emotion recognition",
      "social skill assessment for autism"
    ]
  },
  {
    "topic_id": 69,
    "size": 20,
    "keywords": [
      "eegbased emotion",
      "recognition eegbased",
      "eegbased",
      "emotion recognition",
      "recognition",
      "emotion"
    ],
    "examples": [
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition",
      "eeg-based emotion recognition"
    ],
    "topic_fingerprint": "1ac2a92a4d4d99a78ea0f8ae36aa8409ef6c688e5593fa28275a51d6d85391a2",
    "l2_name": "EEG-based Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states using electroencephalogram (EEG) signals.",
    "aliases": [
      "eegbased emotion recognition",
      "emotion recognition eegbased",
      "eeg emotion recognition",
      "eeg-based affect recognition",
      "electroencephalogram-based emotion classification"
    ]
  },
  {
    "topic_id": 70,
    "size": 20,
    "keywords": [
      "automatic depression",
      "automatic",
      "level",
      "depression level",
      "prediction automatic",
      "detection automatic",
      "level prediction",
      "depression",
      "level detection",
      "severity prediction"
    ],
    "examples": [
      "automatic deception detection",
      "automatic engagement level prediction",
      "automatic depression level detection",
      "automatic depression detection",
      "automatic depression level detection",
      "automatic depression level detection",
      "automatic depression level prediction",
      "adolescent suicidal ideation prediction"
    ],
    "topic_fingerprint": "f04f369dbdff3c41b29d030eee618150664d343b57972afc718ca095ca4ee6f4",
    "l2_name": "Automatic Depression Level Prediction",
    "definition": "The use of computational methods to automatically detect, assess, or predict the severity level of depression in individuals.",
    "aliases": [
      "automatic depression detection",
      "automatic depression severity prediction",
      "depression level detection",
      "automatic depression assessment",
      "depression severity diagnosis",
      "automatic depression level assessment",
      "daily depression score prediction"
    ]
  },
  {
    "topic_id": 71,
    "size": 20,
    "keywords": [
      "small",
      "small sample",
      "limitation",
      "sample",
      "dataset",
      "size",
      "sample size",
      "limitation small",
      "dataset limitation",
      "problem small"
    ],
    "examples": [
      "survey application problems",
      "small sample size challenge",
      "small sample size challenge",
      "single database evaluation limitation",
      "single database evaluation limitation",
      "single-source data limitation",
      "small-scale dataset limitations",
      "small-scale dataset limitation"
    ],
    "topic_fingerprint": "40b56c4325e5642e9dee96d616c2b6616466a23c2320dd1091bd10d36422f79a",
    "l2_name": "Small Sample Size",
    "definition": "A research limitation arising from the use of a dataset with insufficient observations to ensure statistical power or generalizability.",
    "aliases": [
      "small sample",
      "limited sample size",
      "small-scale dataset",
      "single-source data limitation",
      "single database evaluation",
      "small data sample",
      "dataset size limitation",
      "insufficient sample size"
    ]
  },
  {
    "topic_id": 72,
    "size": 20,
    "keywords": [
      "interference",
      "noise reduction",
      "head movement",
      "head",
      "noise",
      "reduction",
      "movement noise",
      "movement",
      "movement interference",
      "irrelevant"
    ],
    "examples": [
      "noise and outlier reduction",
      "classification noise reduction",
      "garner interference effect",
      "head movement noise reduction",
      "head movement noise reduction",
      "head movement noise reduction",
      "noise reduction in eeg",
      "wild scenario interference handling"
    ],
    "topic_fingerprint": "fd6204b68db1148850846e76d526a93c3af2823f4ff0f17da76942a6e5d519bd",
    "l2_name": "Noise and Interference Reduction",
    "definition": "The mitigation of unwanted signal artifacts, outliers, and external disturbances such as head movement to improve data quality and classification accuracy.",
    "aliases": [
      "noise reduction",
      "interference mitigation",
      "head movement noise reduction",
      "outlier reduction",
      "movement interference handling",
      "classification noise reduction",
      "signal artifact removal",
      "irrelevant noise filtering"
    ]
  },
  {
    "topic_id": 73,
    "size": 20,
    "keywords": [
      "neural",
      "basis",
      "correlates",
      "neural basis",
      "neural correlates",
      "emotional intelligence",
      "intelligence",
      "basis emotional",
      "neuroanatomical",
      "correlates social"
    ],
    "examples": [
      "neurogenetic susceptibility mechanisms",
      "neural basis of life satisfaction",
      "neuroanatomical basis of emotional intelligence",
      "neural correlates of social well-being",
      "eudaimonic well-being neural correlates",
      "happy life neural correlates",
      "neural basis of insight emotions",
      "neural basis of mind-body interaction"
    ],
    "topic_fingerprint": "9858157edd46c0e453d2345cc43619d9bbd0a8d9cffa35c001278d0fb459cd7c",
    "l2_name": "Neural Basis of Well-being",
    "definition": "The study of neuroanatomical structures and neural correlates underlying emotional intelligence, social functioning, and various dimensions of psychological well-being.",
    "aliases": [
      "neural basis",
      "neural correlates",
      "neuroanatomical basis",
      "neural basis of emotional intelligence",
      "neural correlates of social well-being",
      "eudaimonic well-being neural correlates",
      "neural basis of emotion",
      "neuroanatomical correlates"
    ]
  },
  {
    "topic_id": 74,
    "size": 20,
    "keywords": [
      "major",
      "depressive disorder",
      "major depressive",
      "depressive",
      "disorder",
      "pathology",
      "disorder pathology",
      "detection major",
      "treatment",
      "disorder recognition"
    ],
    "examples": [
      "late-life depression pathophysiology",
      "neuropsychological impairment in mdd",
      "self-processing deficits in depression",
      "major depression pathology",
      "major depressive disorder pathology",
      "major depressive disorder treatment response",
      "depressive disorder treatment",
      "major depressive disorder pathophysiology"
    ],
    "topic_fingerprint": "15f66930e7f91607daba71c1ef8f913a7e48ad360e92ac44711645764cf7325d",
    "l2_name": "Major Depressive Disorder",
    "definition": "A clinical condition characterized by persistent low mood, loss of interest, and significant impairment in daily functioning due to underlying pathophysiological mechanisms.",
    "aliases": [
      "MDD",
      "major depression",
      "clinical depression",
      "unipolar depression",
      "depressive disorder",
      "major depressive episode"
    ]
  },
  {
    "topic_id": 75,
    "size": 20,
    "keywords": [
      "humor",
      "humor detection",
      "crosscultural humor",
      "detection crosscultural",
      "detection task",
      "crosscultural",
      "sarcasm",
      "multimodal humor",
      "sarcasm detection",
      "task humor"
    ],
    "examples": [
      "humor detection task",
      "multimodal humor detection",
      "multimodal humor detection",
      "multimodal humor detection",
      "cross-cultural humor detection",
      "sarcasm detection",
      "humor detection task",
      "cross-cultural humor detection"
    ],
    "topic_fingerprint": "123bd2e21e340af71205878aed16c81f5ef024fdf6b580897b2a2a83d1552c77",
    "l2_name": "Humor Detection",
    "definition": "The computational task of automatically identifying and classifying humorous content, including sarcasm and cross-cultural variations, often across multimodal data.",
    "aliases": [
      "humor detection task",
      "sarcasm detection",
      "cross-cultural humor detection",
      "multimodal humor detection",
      "humour detection",
      "sarcasm identification",
      "computational humor"
    ]
  },
  {
    "topic_id": 76,
    "size": 20,
    "keywords": [
      "microexpression detection",
      "automatic microexpression",
      "microexpression",
      "analysis microexpression",
      "microexpression analysis",
      "extraction microexpression",
      "microexpression feature",
      "microexpression model",
      "frame inference",
      "microexpression key"
    ],
    "examples": [
      "automatic micro-expression analysis evaluation",
      "micro-expression detection task",
      "micro-expression detection",
      "automatic micro-expression analysis",
      "micro-expression detection task",
      "micro-expression detection task",
      "micro-expression detection",
      "automatic micro-expression analysis"
    ],
    "topic_fingerprint": "26a20826eb2e98873d30c91a68f262dc6c6b6b86eecf6775a13ee1e4d37d0136",
    "l2_name": "Micro-expression Detection",
    "definition": "The automated identification and analysis of brief, involuntary facial expressions that reveal concealed emotions.",
    "aliases": [
      "automatic microexpression detection",
      "microexpression analysis",
      "micro-expression recognition",
      "ME detection",
      "microexpression feature extraction",
      "automatic micro-expression analysis"
    ]
  },
  {
    "topic_id": 77,
    "size": 19,
    "keywords": [
      "temporal",
      "temporal emotion",
      "analysis temporal",
      "spatiotemporal emotion",
      "emotion mapping",
      "spatiotemporal",
      "modeling temporal",
      "mapping",
      "temporal metaphor",
      "temporal duration"
    ],
    "examples": [
      "temporal emotion modeling",
      "temporal emotion analysis",
      "temporal emotion analysis",
      "temporal emotion trend analysis",
      "temporal duration effects",
      "diurnal emotion rhythm analysis",
      "temporal emotion dynamics modeling",
      "temporal emotion analysis"
    ],
    "topic_fingerprint": "e0d99a1caaef035263348c500ee669575487289b9399a36db45f52344ee8590b",
    "l2_name": "Temporal Emotion Analysis",
    "definition": "The study of how emotions evolve, fluctuate, and are represented across time dimensions including duration, rhythm, and spatiotemporal contexts.",
    "aliases": [
      "temporal emotion modeling",
      "spatiotemporal emotion analysis",
      "emotion dynamics",
      "temporal emotion mapping",
      "diurnal emotion rhythm",
      "temporal duration effects",
      "time-dependent emotion analysis"
    ]
  },
  {
    "topic_id": 78,
    "size": 19,
    "keywords": [
      "multimodal feature",
      "feature fusion",
      "fusion",
      "feature",
      "feature integration",
      "recognition limitation",
      "fusion comparison",
      "single modality",
      "complementarity",
      "comparison"
    ],
    "examples": [
      "multimodal feature fusion",
      "multimodal feature integration challenge",
      "multimodal feature fusion comparison",
      "multimodal feature fusion comparison",
      "high-order feature integration",
      "multi-modal information representation",
      "multimodal feature fusion",
      "multimodal feature fusion"
    ],
    "topic_fingerprint": "54b818c1f84e66a18176c7f7993f76f1c27a9be73d89bdee5f27d526a3d06c06",
    "l2_name": "Multimodal Feature Fusion",
    "definition": "The process of integrating complementary information from multiple modalities to overcome single-modality limitations and enhance recognition performance.",
    "aliases": [
      "feature fusion",
      "multimodal feature integration",
      "feature integration",
      "multi-modal information representation",
      "intermediate representation fusion",
      "multi-task feature integration",
      "fusion comparison",
      "high-order feature integration"
    ]
  },
  {
    "topic_id": 79,
    "size": 18,
    "keywords": [
      "label",
      "noisy label",
      "noisy",
      "label noise",
      "noise management",
      "label handling",
      "management",
      "management label",
      "noise",
      "handling"
    ],
    "examples": [
      "automatic corpus labeling",
      "missing label prediction",
      "noisy label handling",
      "missing label handling",
      "noisy social image data",
      "inference without gold labels",
      "label noise management",
      "label noise management"
    ],
    "topic_fingerprint": "0a0b11e59bd779777917549fb3aaf084bb4fb6cca4d9f0ac885b499e8a5830d6",
    "l2_name": "Noisy Label Handling",
    "definition": "The study of methods to detect, correct, or robustly train models in the presence of incorrect, missing, or unreliable annotations in datasets.",
    "aliases": [
      "noisy label management",
      "label noise handling",
      "noisy label learning",
      "label noise correction",
      "handling noisy labels",
      "missing label prediction",
      "automatic corpus labeling",
      "inference without gold labels"
    ]
  },
  {
    "topic_id": 80,
    "size": 18,
    "keywords": [
      "multiview",
      "multiview facial",
      "recognition multiview",
      "analysis multiview",
      "classification multiview",
      "multiview expression",
      "arbitrary",
      "arbitrary view",
      "multiview emotion",
      "view"
    ],
    "examples": [
      "arbitrary view emotion recognition",
      "arbitrary view facial expression recognition",
      "multi-view facial expression recognition",
      "multi-view expression analysis",
      "multi-view emotion classification",
      "multi-view facial expression recognition",
      "multi-view facial expression recognition",
      "multi-view facial expression recognition"
    ],
    "topic_fingerprint": "beee6891f1f62d91ed4393a2e646ccf3f8b9ee57ca7eec1576c253220cd01bc3",
    "l2_name": "Multi-view Facial Expression Recognition",
    "definition": "The task of identifying and classifying human facial expressions from images or videos captured from multiple or arbitrary camera viewpoints.",
    "aliases": [
      "multi-view expression recognition",
      "multi-view emotion recognition",
      "arbitrary view facial expression recognition",
      "multi-view facial expression analysis",
      "multi-view expression classification",
      "multi-view emotion classification",
      "multiview facial expression recognition",
      "multiview expression analysis"
    ]
  },
  {
    "topic_id": 81,
    "size": 18,
    "keywords": [
      "largescale",
      "analysis largescale",
      "emotion analysis",
      "analysis multimodal",
      "dataset collection",
      "analysis multifactor",
      "multimedia",
      "collection",
      "multifactor emotion",
      "multifactor"
    ],
    "examples": [
      "multimodal emotion analysis",
      "multimodal emotion analysis",
      "multi-modal emotion analysis",
      "multi-modal emotion analysis",
      "multi-factor emotion analysis",
      "multi-factor emotion analysis",
      "multifactorial emotion influence analysis",
      "large-scale voice data analysis"
    ],
    "topic_fingerprint": "9073566d02b0b0a2d883c30367d7593995a40145126dde81e7da07ace2aa0309",
    "l2_name": "Multimodal Emotion Analysis",
    "definition": "The computational analysis of human emotions by integrating and processing multiple data modalities such as text, audio, and visual cues from large-scale multimedia datasets.",
    "aliases": [
      "multimodal emotion analysis",
      "multi-modal emotion analysis",
      "multi-factor emotion analysis",
      "multifactorial emotion influence analysis",
      "large-scale heterogeneous multimedia analysis",
      "analysis multimodal",
      "emotion analysis",
      "multifactor emotion"
    ]
  },
  {
    "topic_id": 82,
    "size": 18,
    "keywords": [
      "spotting",
      "spotting microexpression",
      "microexpression spotting",
      "expression spotting",
      "macro micro",
      "macro",
      "spotting macro",
      "micro expression",
      "micro",
      "interval"
    ],
    "examples": [
      "facial expression spotting",
      "micro-expression spotting",
      "micro-expression spotting evaluation",
      "sub-second interval perception",
      "facial micro-expression spotting",
      "micro-expression spotting",
      "macro and micro expression spotting",
      "micro-expression interval spotting"
    ],
    "topic_fingerprint": "cc2ec6a8144328904dc4baf3ac037a2a4d2d93a114ad6e47c7a7852d9278eca6",
    "l2_name": "Micro-expression Spotting",
    "definition": "The task of detecting and localizing the temporal intervals of fleeting, involuntary facial micro-expressions within video sequences.",
    "aliases": [
      "facial expression spotting",
      "micro-expression spotting evaluation",
      "sub-second interval perception",
      "facial micro-expression spotting",
      "macro and micro expression spotting",
      "micro-expression interval spotting",
      "in-the-wild micro-expression spotting",
      "spotting microexpression",
      "expression spotting",
      "spotting macro"
    ]
  },
  {
    "topic_id": 83,
    "size": 17,
    "keywords": [
      "modulation",
      "emotional modulation",
      "emotional context",
      "attention",
      "context effect",
      "attention modulation",
      "context",
      "effect",
      "spatial attention",
      "sensitivity affect"
    ],
    "examples": [
      "emotion modulated identity recognition",
      "emotion-driven attention modulation",
      "contextual sensitivity in affect",
      "emotional context effect",
      "spatial attention modulation by emotion",
      "self-reference effect modulation",
      "automatic attention attraction",
      "emotional context modulation effects"
    ],
    "topic_fingerprint": "95924df8bc1a11e37c4ea60c6af85431cfb6ad6a718d22b4454dd978eefe99fa",
    "l2_name": "Emotional Modulation",
    "definition": "The process by which emotional states or contexts influence cognitive functions such as attention, perception, and memory.",
    "aliases": [
      "emotion modulation",
      "affective modulation",
      "emotional context effect",
      "emotion-driven attention",
      "contextual sensitivity in affect",
      "emotional modulation of cognition"
    ]
  },
  {
    "topic_id": 84,
    "size": 17,
    "keywords": [
      "emotion distribution",
      "distribution",
      "distribution learning",
      "distribution prediction",
      "learning visual",
      "prediction discrete",
      "discrete probability",
      "probability distribution",
      "probability",
      "discrete"
    ],
    "examples": [
      "discrete emotion distribution prediction",
      "discrete emotion distribution prediction",
      "discrete probability distribution modeling",
      "discrete probability distribution modeling",
      "image emotion distribution prediction",
      "image emotion distribution prediction",
      "discrete probability distribution prediction",
      "discrete emotion distribution prediction"
    ],
    "topic_fingerprint": "e9e70876581ed49d20f342d62342fa0c0f5b7101de0dfe0778bf2c9e8bb4f997",
    "l2_name": "Emotion Distribution Learning",
    "definition": "The task of predicting a probability distribution over discrete emotion categories rather than a single dominant label to capture the ambiguity and subjectivity of emotional perception.",
    "aliases": [
      "emotion distribution prediction",
      "discrete emotion distribution learning",
      "visual emotion distribution learning",
      "image emotion distribution prediction",
      "text emotion distribution learning",
      "distribution learning for emotions",
      "probabilistic emotion recognition"
    ]
  },
  {
    "topic_id": 85,
    "size": 16,
    "keywords": [
      "recognition dimensional",
      "dimensional",
      "dimensional emotion",
      "modeling dimensional",
      "dimensional image",
      "scoring",
      "analysis dimensional",
      "emotion scoring",
      "detection dimensional",
      "dimensional speech"
    ],
    "examples": [
      "dimensional emotion recognition",
      "dimensional emotion recognition",
      "dimensional emotion recognition",
      "dimensional image emotion analysis",
      "dimensional emotion modeling",
      "dimensional emotion recognition",
      "dimensional emotion recognition",
      "dimensional emotion estimation"
    ],
    "topic_fingerprint": "3625d56e4d14ce1671f41fa85ffccde4bc0d9a86310130ec0273db120010b57f",
    "l2_name": "Dimensional Emotion Recognition",
    "definition": "The computational task of modeling and detecting human emotions using continuous dimensional scales such as valence, arousal, and dominance rather than discrete categories.",
    "aliases": [
      "dimensional emotion analysis",
      "dimensional emotion modeling",
      "dimensional emotion estimation",
      "dimensional emotion scoring",
      "continuous emotion recognition",
      "dimensional speech emotion recognition",
      "dimensional image emotion analysis"
    ]
  },
  {
    "topic_id": 86,
    "size": 16,
    "keywords": [
      "generalization",
      "crossdomain generalization",
      "unseen",
      "generalization unseen",
      "unseen subject",
      "generalization challenge",
      "subject generalization",
      "crossdataset generalization",
      "generalization crossdomain",
      "crossdomain"
    ],
    "examples": [
      "cross-document evidence integration",
      "poor generalization ability",
      "cross-domain generalization gap",
      "cross-domain feature extraction",
      "domain generalization for unseen conditions",
      "limited feature generalization capability",
      "cross-dataset generalization challenge",
      "cross-dataset generalization challenge"
    ],
    "topic_fingerprint": "ce27d081a8fe0ffd391a67616e54fcb71b9f41172e00ecfd469a177208277e7a",
    "l2_name": "Cross-Domain Generalization",
    "definition": "The challenge of maintaining model performance when applied to unseen domains, subjects, or datasets that differ from the training distribution.",
    "aliases": [
      "domain generalization",
      "cross-dataset generalization",
      "unseen subject generalization",
      "generalization to unseen conditions",
      "cross-domain gap",
      "subject generalization",
      "generalization challenge"
    ]
  },
  {
    "topic_id": 87,
    "size": 16,
    "keywords": [
      "modality robustness",
      "robustness",
      "robustness noise",
      "noise modality",
      "noise",
      "modality",
      "model",
      "robustness improvement",
      "robustness evaluation",
      "model robustness"
    ],
    "examples": [
      "feature robustness evaluation",
      "model robustness improvement",
      "model robustness improvement",
      "robustness to outliers and noise",
      "modality-wise uncertainty quantification",
      "unreliable model performance metrics",
      "modality robustness under noise",
      "modality robustness under noise"
    ],
    "topic_fingerprint": "411bd999830a99c765e5f7907d3a69aa6ed9927af25d4c986d17b8a0e6b0a259",
    "l2_name": "Modality Robustness",
    "definition": "The capability of a model to maintain reliable performance and accurate predictions across different data modalities despite the presence of noise, outliers, or distributional shifts.",
    "aliases": [
      "robustness to noise",
      "modality-wise robustness",
      "cross-modal robustness",
      "noise robustness",
      "model robustness evaluation",
      "robustness improvement",
      "uncertainty quantification under noise"
    ]
  },
  {
    "topic_id": 88,
    "size": 16,
    "keywords": [
      "conflict",
      "emotional conflict",
      "conflict resolution",
      "resolution emotional",
      "resolution",
      "communication",
      "difficulties",
      "conflict control",
      "conflict adaptation",
      "communication analysis"
    ],
    "examples": [
      "emotional conflict adaptation",
      "nonverbal communication analysis",
      "cognitive conflict resolution",
      "emotional conflict control",
      "theoretical conflict resolution",
      "emotional conflict control",
      "reform-induced psychological conflict",
      "communication barriers"
    ],
    "topic_fingerprint": "466b0459724474b4deb794a959b5e9ec58c237ae3025e92783c62411ccf27784",
    "l2_name": "Emotional Conflict Resolution",
    "definition": "The process of identifying, analyzing, and resolving interpersonal or intrapersonal conflicts driven by emotional factors through communication and adaptation strategies.",
    "aliases": [
      "emotional conflict",
      "conflict resolution",
      "emotional conflict control",
      "conflict adaptation",
      "communication difficulties",
      "emotional tradeoff difficulties",
      "collaborative negotiation difficulties"
    ]
  },
  {
    "topic_id": 89,
    "size": 16,
    "keywords": [
      "deep",
      "deep learning",
      "limitation deep",
      "data limitation",
      "learning data",
      "limitation",
      "learning",
      "performance degradation",
      "network performance",
      "deep network"
    ],
    "examples": [
      "data limitation in deep learning",
      "data limitation in deep learning",
      "data limitation in deep learning",
      "data limitation in deep learning",
      "data limitation in deep learning",
      "data limitation in deep learning",
      "deep network performance degradation",
      "deep network performance degradation"
    ],
    "topic_fingerprint": "a2ba10a05d4fba4aac3a0aabca7c5cba0f0374327ae3071ea875005ade9381a4",
    "l2_name": "Data Limitation in Deep Learning",
    "definition": "The challenge where deep learning models suffer from performance degradation or inefficiency due to insufficient, poor-quality, or imbalanced training data.",
    "aliases": [
      "data scarcity in deep learning",
      "limited training data",
      "data insufficiency",
      "small data problem",
      "data limitation",
      "learning data limitation",
      "deep learning data constraints"
    ]
  },
  {
    "topic_id": 90,
    "size": 16,
    "keywords": [
      "recognition gestures",
      "gestures emotion",
      "uncertainty emotion",
      "images emotion",
      "images",
      "recognition images",
      "hci emotion",
      "recognition hci",
      "gestures",
      "recognition uncertainty"
    ],
    "examples": [
      "emotion recognition with history",
      "emotion recognition task",
      "emotion recognition task",
      "emotion recognition from video",
      "emotion recognition in challenging environments",
      "emotion recognition in videos",
      "emotion recognition in hci",
      "emotion recognition from images"
    ],
    "topic_fingerprint": "9cbd88ad8f8bedd35256d06c139cceab19c7c79e8433a81e03072f763f12c6ba",
    "l2_name": "Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states from multimodal inputs such as images, videos, gestures, and contextual uncertainty.",
    "aliases": [
      "Affective Computing",
      "Emotion Detection",
      "Facial Expression Recognition",
      "Gesture-based Emotion Recognition",
      "Multimodal Emotion Recognition",
      "HCI Emotion Analysis"
    ]
  },
  {
    "topic_id": 91,
    "size": 16,
    "keywords": [
      "visual",
      "visual emotion",
      "painting emotion",
      "recognition visual",
      "painting",
      "master painting",
      "master",
      "visual art",
      "analysis master",
      "emotion analysis"
    ],
    "examples": [
      "master painting emotion analysis",
      "master painting emotion analysis",
      "master painting emotion analysis",
      "visual emotion analysis",
      "visual emotion recognition",
      "visual art emotion recognition",
      "emotion-aware visual perception",
      "system interpretability in emotion"
    ],
    "topic_fingerprint": "957abb8ae0190005f9d1d7ef6a2ff40eac186dbacf75d53296ae7d4125c4f7b3",
    "l2_name": "Visual Emotion Analysis",
    "definition": "The computational task of recognizing, interpreting, and analyzing emotional content within visual art such as paintings.",
    "aliases": [
      "visual emotion recognition",
      "painting emotion analysis",
      "master painting emotion analysis",
      "visual art emotion recognition",
      "emotion-aware visual perception",
      "emotional visual art comprehension",
      "source-free visual emotion adaptation",
      "visual emotion analysis generalizability"
    ]
  },
  {
    "topic_id": 92,
    "size": 15,
    "keywords": [
      "personalized",
      "prediction personalized",
      "personalized emotion",
      "personalized image",
      "perception personalized",
      "emotion perception",
      "emotion prediction",
      "perception",
      "perception prediction",
      "image emotion"
    ],
    "examples": [
      "personalized emotion perception",
      "personalized emotion perception prediction",
      "personalized emotion perception prediction",
      "personalized emotion perception prediction",
      "personalized image emotion prediction",
      "personalized image emotion perception",
      "personalized image emotion prediction",
      "personalized image emotion prediction"
    ],
    "topic_fingerprint": "fcfb0ff06accdc7600f8a313f65fe78984f778bb0f1dfc414f9a5d6ee10e274e",
    "l2_name": "Personalized Emotion Perception",
    "definition": "The computational task of predicting or analyzing emotional responses to stimuli, such as images, tailored to the specific characteristics and preferences of individual users.",
    "aliases": [
      "personalized emotion prediction",
      "individual-specific emotion prediction",
      "personalized image emotion perception",
      "personalized image emotion prediction",
      "user-specific emotion analysis",
      "adaptive emotion recognition"
    ]
  },
  {
    "topic_id": 93,
    "size": 15,
    "keywords": [
      "recognition crosssubject",
      "crosssubject",
      "crosssubject eeg",
      "crosssubject emotion",
      "eeg emotion",
      "eeg",
      "recognition intersubject",
      "intersubject",
      "multisubject emotion",
      "multisubject"
    ],
    "examples": [
      "multi-subject emotion recognition",
      "inter-subject emotion recognition",
      "cross-subject eeg emotion recognition",
      "cross-subject eeg emotion recognition",
      "cross-subject eeg emotion recognition",
      "cross-subject eeg emotion recognition",
      "cross-subject eeg emotion recognition",
      "cross-subject emotion recognition"
    ],
    "topic_fingerprint": "98ffd3e48df081a8379d02791efb32e811c9fc8551ec9a6ef5879fe05e9134df",
    "l2_name": "Cross-Subject EEG Emotion Recognition",
    "definition": "The problem of recognizing human emotions from EEG signals by training models on data from one set of subjects and testing on unseen subjects to ensure generalization across individuals.",
    "aliases": [
      "cross-subject emotion recognition",
      "inter-subject emotion recognition",
      "multi-subject emotion recognition",
      "cross-subject EEG",
      "intersubject generalization",
      "cross-subject affective computing",
      "subject-independent emotion recognition"
    ]
  },
  {
    "topic_id": 94,
    "size": 15,
    "keywords": [
      "recognition conversation",
      "conversation emotion",
      "conversations emotion",
      "conversations",
      "conversation",
      "recognition conversations",
      "multiparty",
      "recognition multiparty",
      "noisy speech",
      "multiparty conversations"
    ],
    "examples": [
      "emotion recognition from noisy speech",
      "emotion recognition from speech",
      "emotion recognition in conversations",
      "emotion recognition in conversation",
      "emotion recognition in conversations",
      "emotion recognition in conversation",
      "emotion recognition in conversation",
      "emotion recognition in multi-party conversations"
    ],
    "topic_fingerprint": "22af3dafdf26c9c98b8eb78d23694e99f1fc127b714448257a2466b3bc83d591",
    "l2_name": "Conversational Emotion Recognition",
    "definition": "The task of identifying and classifying emotional states from speech or text within single-party or multi-party conversational contexts, often under challenging conditions like noise.",
    "aliases": [
      "emotion recognition in conversation",
      "ERC",
      "conversational emotion detection",
      "emotion recognition in conversations",
      "multi-party emotion recognition",
      "emotion recognition from noisy speech",
      "conversation emotion recognition"
    ]
  },
  {
    "topic_id": 95,
    "size": 15,
    "keywords": [
      "spotting task",
      "microexpression spotting",
      "spotting",
      "task microexpression",
      "task",
      "microexpression",
      "task facial",
      "expression spotting",
      "facial expression",
      "expression"
    ],
    "examples": [
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task",
      "micro-expression spotting task"
    ],
    "topic_fingerprint": "29a472c16873e51daf593d70ae8af4dd29322dc06b60852be27e449d723e2eee",
    "l2_name": "Micro-expression Spotting",
    "definition": "The task of detecting and localizing the temporal onset and offset of micro-expressions in video sequences.",
    "aliases": [
      "micro-expression spotting task",
      "microexpression spotting",
      "facial expression spotting",
      "expression spotting task",
      "spotting task",
      "task microexpression",
      "task facial expression"
    ]
  },
  {
    "topic_id": 96,
    "size": 14,
    "keywords": [
      "memory",
      "memory emotion",
      "item",
      "emotional memory",
      "source memory",
      "source",
      "emotion effect",
      "emotion effects",
      "item memory",
      "effects"
    ],
    "examples": [
      "emotion-memory interaction",
      "encoding versus retrieval effects",
      "attitude-memory dissociation analysis",
      "emotion-induced memory consolidation",
      "gender effects on memory",
      "item versus source memory",
      "emotion effects on item memory",
      "emotion effects on source memory"
    ],
    "topic_fingerprint": "18d0120dcf32280ebdf41028607685d9a8e7595273621b109d2cbc234b64f4a2",
    "l2_name": "Emotional Memory",
    "definition": "The study of how emotion influences the encoding, consolidation, and retrieval of item and source memory.",
    "aliases": [
      "emotion-memory interaction",
      "emotion effects on memory",
      "emotional memory effects",
      "emotion-induced memory",
      "affective memory"
    ]
  },
  {
    "topic_id": 97,
    "size": 14,
    "keywords": [
      "domain shift",
      "shift",
      "domain",
      "shift emotion",
      "data domain",
      "analysis domain",
      "shift affective",
      "emotion data",
      "crosscultural domain",
      "computing adaptive"
    ],
    "examples": [
      "domain shift in emotion analysis",
      "domain mismatch in emotion data",
      "domain shift in emotion data",
      "domain shift in facial expressions",
      "domain shift in subtle dynamics",
      "domain shift in emotion analysis",
      "tourism domain emotion detection",
      "domain shift in emotion data"
    ],
    "topic_fingerprint": "883a9a7cd4f6bbae2d0f614b3b4cfc097d496433141cfb4dbafeb3b0809ced78",
    "l2_name": "Domain Shift in Emotion Analysis",
    "definition": "The challenge of performance degradation in affective computing models when training and testing data originate from different distributions, such as varying cultures, sensors, or environmental conditions.",
    "aliases": [
      "domain shift",
      "domain mismatch",
      "cross-domain emotion analysis",
      "adaptive domain shift",
      "shift in affective data",
      "data domain shift",
      "cross-cultural domain shift"
    ]
  },
  {
    "topic_id": 98,
    "size": 14,
    "keywords": [
      "subtle emotion",
      "subtle",
      "emotion detection",
      "detection subtle",
      "perception subtle",
      "longterm emotion",
      "subtle visual",
      "detection longterm",
      "longterm",
      "recognition short"
    ],
    "examples": [
      "subtle emotion recognition task",
      "deep event emotion analysis",
      "subtle emotion detection",
      "public emotion detection",
      "long-term emotion detection",
      "long-term emotion detection",
      "subtle emotion recognition",
      "short duration emotion analysis"
    ],
    "topic_fingerprint": "8a6b03035b2d5bd4e4b1d0d0098d83606123df11305db0d66fc334e3d248580e",
    "l2_name": "Subtle Emotion Detection",
    "definition": "The computational task of identifying and analyzing low-intensity, short-duration, or long-term emotional states from visual and behavioral cues that are difficult for humans to perceive.",
    "aliases": [
      "subtle emotion recognition",
      "micro-expression detection",
      "long-term emotion detection",
      "short duration emotion analysis",
      "subtle visual emotion perception",
      "deep event emotion analysis",
      "understanding subtle user emotional states",
      "public emotion detection"
    ]
  },
  {
    "topic_id": 99,
    "size": 14,
    "keywords": [
      "4d facial",
      "4d",
      "recognition 4d",
      "recognition 3d4d",
      "3d4d facial",
      "3d4d",
      "facial affect",
      "expression recognition",
      "affect recognition",
      "facial expression"
    ],
    "examples": [
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition",
      "4d facial expression recognition"
    ],
    "topic_fingerprint": "e5cc915a9fdb6d34e91a2f152b080a7e3e588c317a65e1592f70be20b798ccaf",
    "l2_name": "4D Facial Expression Recognition",
    "definition": "The automated analysis and classification of dynamic facial expressions and affective states using time-sequenced three-dimensional data.",
    "aliases": [
      "4d facial recognition",
      "3d/4d facial expression recognition",
      "4d facial affect recognition",
      "3d4d facial expression recognition",
      "dynamic 3d facial expression recognition",
      "spatiotemporal facial expression analysis"
    ]
  },
  {
    "topic_id": 100,
    "size": 14,
    "keywords": [
      "speaker identity",
      "speaker",
      "identity",
      "bias",
      "bias mitigation",
      "identity bias",
      "mitigation speaker",
      "identity distinction",
      "features speaker",
      "identity mismatch"
    ],
    "examples": [
      "rating data sparsity mitigation",
      "speaker identity distinction",
      "speaker identity distinction",
      "speaker identity mismatch",
      "speaker identity mismatch",
      "subject identity bias mitigation",
      "cross-database bias mitigation",
      "speaker bias in features"
    ],
    "topic_fingerprint": "efc2c875d4bbc55d9c1e1d7fd393b0e08ce9952948ae416513706c48f5cb258b",
    "l2_name": "Speaker Identity Bias",
    "definition": "The problem of spurious correlations between speaker identity features and target labels that degrade model generalization and fairness.",
    "aliases": [
      "speaker bias",
      "identity bias",
      "speaker identity mismatch",
      "speaker identity distinction",
      "bias mitigation speaker",
      "subject identity bias",
      "spurious speaker bias",
      "speaker-dependent bias"
    ]
  },
  {
    "topic_id": 101,
    "size": 14,
    "keywords": [
      "modality",
      "missing",
      "missing modality",
      "modality handling",
      "incomplete modality",
      "handling",
      "feature missing",
      "random modality",
      "modality feature",
      "settings"
    ],
    "examples": [
      "variable-length segment identification",
      "missing modality handling",
      "handling criteria interactions",
      "missing modality handling",
      "handling uncertain modality absence",
      "incomplete modality handling",
      "incomplete modality settings",
      "incomplete modality settings"
    ],
    "topic_fingerprint": "2206be419bb968daf229c9e337f6d57db4d718ffe8f414d0fe752889c59b5509",
    "l2_name": "Missing Modality Handling",
    "definition": "The process of managing, reconstructing, or adapting to incomplete or absent data modalities within a multimodal system.",
    "aliases": [
      "incomplete modality handling",
      "missing modality reconstruction",
      "modality missing",
      "feature missing",
      "handling uncertain modality absence",
      "random modality feature missing",
      "incomplete modality settings"
    ]
  },
  {
    "topic_id": 102,
    "size": 13,
    "keywords": [
      "noise robustness",
      "robustness",
      "noise",
      "robustness analysis",
      "analysis noise",
      "robustness complex",
      "complex environments",
      "robustness recognition",
      "environments",
      "recognition robustness"
    ],
    "examples": [
      "robustness to increasing noise levels",
      "robust me location detection",
      "low recognition robustness",
      "recognition system robustness",
      "noise robustness enhancement",
      "noise robustness analysis",
      "noise robustness analysis",
      "noise robustness analysis"
    ],
    "topic_fingerprint": "5cf16edba2af58a9b4b27d396dfa5567c95ea1a3b1db9add98cadfbde0243b36",
    "l2_name": "Noise Robustness",
    "definition": "The capability of a system to maintain accurate performance and recognition accuracy despite the presence of noise or operation in complex environments.",
    "aliases": [
      "robustness to noise",
      "noise robustness analysis",
      "recognition robustness",
      "robustness in complex environments",
      "noise tolerance",
      "environmental robustness"
    ]
  },
  {
    "topic_id": 103,
    "size": 13,
    "keywords": [
      "affective gap",
      "gap",
      "bridging",
      "gap bridging",
      "bridging affective",
      "gap affective",
      "affective",
      "bridging bridging",
      "misalignment crossdomain",
      "macrotomicro intensity"
    ],
    "examples": [
      "affective gap bridging",
      "affective gap bridging",
      "affective gap bridging",
      "affective gap challenge",
      "cultural knowledge transfer gap",
      "affective gap bridging",
      "affective gap bridging",
      "affective gap bridging"
    ],
    "topic_fingerprint": "5fede9f7a2e205db2ef67186d002f471d3ba74a5c0405ea9e46a29482ad57fbb",
    "l2_name": "Affective Gap Bridging",
    "definition": "The challenge of resolving misalignments in emotional intensity and representation between different domains or scales, such as macro-to-micro transitions.",
    "aliases": [
      "affective gap",
      "gap bridging",
      "bridging affective gap",
      "affective space misalignment",
      "macro-to-micro intensity gap",
      "cross-domain affective misalignment",
      "emotional gap bridging"
    ]
  },
  {
    "topic_id": 104,
    "size": 13,
    "keywords": [
      "affective speech",
      "synthesis affective",
      "synthesis",
      "speech analysis",
      "speech synthesis",
      "speech pattern",
      "analysis affective",
      "affective",
      "speech",
      "pattern analysis"
    ],
    "examples": [
      "affective speech synthesis",
      "affective gesture synthesis",
      "affective speech pattern analysis",
      "affective response synthesis",
      "affective speech analysis",
      "affective speech analysis",
      "affective speech model robustness",
      "affective speech synthesis"
    ],
    "topic_fingerprint": "b8d09af855186492b030c2ae39548774402bc50f21685ce0eec69fd1c47bd437",
    "l2_name": "Affective Speech Processing",
    "definition": "The computational analysis and synthesis of speech signals to detect, interpret, and generate emotional content and patterns.",
    "aliases": [
      "affective speech analysis",
      "affective speech synthesis",
      "speech-based affective computing",
      "emotional speech processing",
      "affective pattern analysis",
      "emotion recognition in speech",
      "emotional speech synthesis"
    ]
  },
  {
    "topic_id": 105,
    "size": 13,
    "keywords": [
      "subjectindependent",
      "recognition subjectindependent",
      "subjectindependent emotion",
      "subjectindependent eeg",
      "subjectdependent emotion",
      "subjectdependent",
      "recognition subjectdependent",
      "eeg emotion",
      "eeg",
      "person independent"
    ],
    "examples": [
      "person independent emotion recognition",
      "subject-independent eeg emotion recognition",
      "subject-independent eeg emotion recognition",
      "subject-independent emotion recognition",
      "subject-independent emotion classification",
      "subject-independent emotion recognition",
      "subject-independent emotion recognition",
      "subject-dependent emotion recognition"
    ],
    "topic_fingerprint": "a716059621ea8e178c13379e6408ca1b8f169cbead08ee12f4a09f917c0681be",
    "l2_name": "Subject-Independent Emotion Recognition",
    "definition": "The task of recognizing human emotions from physiological signals or behavioral cues using models trained on data from individuals other than the target user to ensure generalizability across subjects.",
    "aliases": [
      "person independent emotion recognition",
      "subject-independent eeg emotion recognition",
      "subject-independent emotion classification",
      "cross-subject emotion recognition",
      "user-independent emotion recognition",
      "subject-independent affective computing"
    ]
  },
  {
    "topic_id": 106,
    "size": 13,
    "keywords": [
      "computing",
      "affective computing",
      "computing integration",
      "computing design",
      "affective",
      "integration affective",
      "realworld affective",
      "realworld",
      "design",
      "deployment affective"
    ],
    "examples": [
      "affective human computer interaction",
      "affective computing research scope",
      "affective computing integration",
      "affective computing foundation",
      "identity-free affective computing",
      "large-scale affective computing deployment",
      "affective computing integration",
      "affective computing challenges"
    ],
    "topic_fingerprint": "2b3cf9343e7a4f4fd819edc79d58aec090659bd911c390395da1411cc8ba12cb",
    "l2_name": "Affective Computing",
    "definition": "The study and development of systems and devices that can recognize, interpret, process, and simulate human affects.",
    "aliases": [
      "Emotion AI",
      "Affective HCI",
      "Computational Emotion Analysis",
      "Affective Human-Computer Interaction",
      "Real-world Affective Computing"
    ]
  },
  {
    "topic_id": 107,
    "size": 13,
    "keywords": [
      "emotional valence",
      "valence",
      "stimulus",
      "valence assessment",
      "effect child",
      "valencebased",
      "attentional bias",
      "attention valence",
      "valence modulation",
      "valence effect"
    ],
    "examples": [
      "affective stimulus devaluation",
      "stimulus valence difference",
      "emotional valence effects on attention",
      "valence congruency effects",
      "emotional valence processing mechanisms",
      "emotional valence judgment",
      "neutral object valence acquisition",
      "emotional valence effect"
    ],
    "topic_fingerprint": "ba93aa552e31c0b2c61c536fb6bbffd04c948844d4f82e9564bef3e15980534d",
    "l2_name": "Emotional Valence",
    "definition": "The intrinsic attractiveness or averseness of a stimulus that modulates cognitive processes such as attention, judgment, and memory.",
    "aliases": [
      "valence",
      "affective valence",
      "stimulus valence",
      "emotional tone",
      "valence effect",
      "valence modulation",
      "positive and negative valence"
    ]
  },
  {
    "topic_id": 108,
    "size": 13,
    "keywords": [
      "eeg based",
      "based emotion",
      "based",
      "recognition eeg",
      "eeg",
      "tracking eegbased",
      "computing eeg",
      "eye tracking",
      "eeg eye",
      "imagebased eeg"
    ],
    "examples": [
      "eeg signal based emotion recognition",
      "eeg based emotion recognition",
      "eeg based emotion recognition",
      "eeg based emotion recognition",
      "eeg based emotion recognition",
      "eeg based emotion recognition",
      "eeg based emotion recognition",
      "image-based eeg emotion recognition"
    ],
    "topic_fingerprint": "9765141e4a5c7bcd250fcfe29518c549583fe6e1750abd2a1827010c9de4851d",
    "l2_name": "EEG-based Emotion Recognition",
    "definition": "The computational process of identifying and classifying human emotional states using electroencephalogram (EEG) signals, often integrated with eye-tracking or image-based data.",
    "aliases": [
      "EEG emotion recognition",
      "EEG-based affective computing",
      "emotion recognition using EEG",
      "EEG and eye tracking for emotion",
      "image-based EEG emotion recognition",
      "EEG signal based emotion recognition"
    ]
  },
  {
    "topic_id": 109,
    "size": 13,
    "keywords": [
      "macroexpression",
      "expression analysis",
      "viewindependent expression",
      "intertwined",
      "expression scenarios",
      "macroexpression spotting",
      "viewindependent",
      "intertwined expression",
      "analysis macroexpression",
      "scenarios"
    ],
    "examples": [
      "pose invariant expression analysis",
      "automatic versus controlled expression analysis",
      "macroexpression processing differences",
      "view-independent expression analysis",
      "view-independent expression analysis",
      "pose-invariant expression analysis",
      "intertwined expression scenarios",
      "intertwined expression scenarios"
    ],
    "topic_fingerprint": "404d1232845e956491a42c606c21367ddb5ab72c246c20e6959f9108ff6f4991",
    "l2_name": "Macroexpression Analysis",
    "definition": "The computational study of detecting, spotting, and analyzing full-blown facial expressions under challenging real-world conditions such as pose variations, intertwined scenarios, and view independence.",
    "aliases": [
      "macro-expression analysis",
      "macroexpression spotting",
      "view-independent expression analysis",
      "pose-invariant expression analysis",
      "intertwined expression scenarios",
      "automatic macroexpression detection",
      "real-world expression analysis"
    ]
  },
  {
    "topic_id": 110,
    "size": 13,
    "keywords": [
      "continuous dimensional",
      "timecontinuous",
      "timecontinuous emotion",
      "prediction timecontinuous",
      "dimensional emotion",
      "emotion prediction",
      "dimensional",
      "continuous",
      "recognition continuous",
      "prediction continuous"
    ],
    "examples": [
      "continuous dimensional emotion prediction",
      "continuous dimensional emotion recognition",
      "continuous dimensional emotion prediction",
      "continuous dimensional emotion prediction",
      "continuous dimensional emotion recognition",
      "continuous dimensional emotion recognition",
      "continuous dimensional emotion recognition",
      "continuous mimicked emotion prediction"
    ],
    "topic_fingerprint": "776f2edccaad246eff1616d5fcb811da3c632141a3f79c5d2ae3ea36609f0b2f",
    "l2_name": "Continuous Dimensional Emotion Prediction",
    "definition": "The task of predicting emotion states as continuous values along dimensional axes (such as valence and arousal) over a time-continuous sequence.",
    "aliases": [
      "time-continuous emotion prediction",
      "continuous dimensional emotion recognition",
      "dimensional emotion prediction",
      "time-continuous affect recognition",
      "continuous emotion prediction",
      "dimensional affect prediction"
    ]
  },
  {
    "topic_id": 111,
    "size": 12,
    "keywords": [
      "semisupervised",
      "unseen emotion",
      "unseen",
      "classification unseen",
      "unsupervised emotion",
      "semisupervised emotion",
      "supervised emotion",
      "recognition semisupervised",
      "classification tasks",
      "unsupervised"
    ],
    "examples": [
      "unsupervised emotion classification",
      "unseen emotion classification",
      "semi-supervised emotion analysis",
      "unseen emotion classification",
      "unseen emotion classification",
      "supervised emotion classification tasks",
      "supervised emotion classification tasks",
      "unsupervised emotion class prediction"
    ],
    "topic_fingerprint": "c6a2e39be568785c9247780856512a1f2532d4ee3da8bcf08889bee99673f579",
    "l2_name": "Unseen Emotion Classification",
    "definition": "The task of identifying and categorizing emotional states that were not present in the training data, often utilizing semi-supervised or unsupervised learning techniques.",
    "aliases": [
      "unseen emotion recognition",
      "zero-shot emotion classification",
      "semi-supervised emotion analysis",
      "unsupervised emotion classification",
      "open-set emotion recognition",
      "novel emotion detection"
    ]
  },
  {
    "topic_id": 112,
    "size": 12,
    "keywords": [
      "bimodal emotion",
      "bimodal",
      "recognition bimodal",
      "multimodal speech",
      "openvocabulary multimodal",
      "uncertaintyaware",
      "uncertaintyaware multimodal",
      "recognition uncertaintyaware",
      "recognition multimodal",
      "recognition openvocabulary"
    ],
    "examples": [
      "bimodal emotion recognition",
      "bimodal emotion recognition",
      "bimodal emotion recognition",
      "bimodal emotion recognition",
      "multi-modal emotion recognition",
      "multi-modal emotion recognition",
      "multimodal speech emotion recognition",
      "uncertainty-aware multimodal emotion recognition"
    ],
    "topic_fingerprint": "9e82344b4a556d86872224a4e910e05601122c01a0de8a114703d6bb4a4517ca",
    "l2_name": "Multimodal Emotion Recognition",
    "definition": "The computational task of identifying and classifying human emotional states by integrating and analyzing data from multiple modalities such as speech, text, and visual cues.",
    "aliases": [
      "bimodal emotion recognition",
      "multi-modal emotion recognition",
      "multimodal speech emotion recognition",
      "uncertainty-aware multimodal emotion recognition",
      "open-vocabulary multimodal emotion recognition",
      "MER",
      "multimodal affect recognition"
    ]
  },
  {
    "topic_id": 113,
    "size": 12,
    "keywords": [
      "personalized",
      "personalized emotional",
      "recognition personalized",
      "personalized emotion",
      "representation personalized",
      "emotional representation",
      "expression personalized",
      "data privacypreserving",
      "preservation emotion",
      "monitoring personalized"
    ],
    "examples": [
      "personalized emotion recognition",
      "personalized emotion recognition",
      "personalized emotion recognition",
      "personalized emotion recognition",
      "personal music emotion recognition",
      "personalized emotional expression",
      "personalized emotional state monitoring",
      "personalized emotional representation"
    ],
    "topic_fingerprint": "060280b3e6f99570ca00e2152bc6e44726d51268cc288b0bb09d8fe6bbc8ecf1",
    "l2_name": "Personalized Emotion Recognition",
    "definition": "The development of adaptive systems that identify and interpret individual emotional states while addressing challenges in representation, expression monitoring, and data privacy.",
    "aliases": [
      "personalized emotion recognition",
      "personalized emotional recognition",
      "personal emotion recognition",
      "individualized emotion recognition",
      "user-specific emotion recognition",
      "personalized affective computing",
      "privacy-preserving personalized emotion recognition",
      "personalized emotional state monitoring",
      "personalized emotional representation"
    ]
  },
  {
    "topic_id": 114,
    "size": 12,
    "keywords": [
      "spontaneous emotion",
      "spontaneous",
      "spontaneous affect",
      "elicitation spontaneous",
      "elicitation",
      "emotion elicitation",
      "detection spontaneous",
      "analysis spontaneous",
      "classification spontaneous",
      "analysis sudden"
    ],
    "examples": [
      "spontaneous emotion corpus creation",
      "acted and spontaneous emotion classification",
      "spontaneous communication analysis",
      "spontaneous emotion analysis",
      "spontaneous emotion detection",
      "spontaneous emotion detection",
      "spontaneous affect identification",
      "spontaneous emotion elicitation"
    ],
    "topic_fingerprint": "64fd8ec95fb6a441df0d234a819f6555d0418a4d8e16e57a7efaec9a8d41d9c8",
    "l2_name": "Spontaneous Emotion Analysis",
    "definition": "The study and computational processing of naturally occurring emotional expressions, including their elicitation, detection, classification, and corpus creation.",
    "aliases": [
      "spontaneous affect analysis",
      "spontaneous emotion detection",
      "spontaneous emotion classification",
      "spontaneous emotion elicitation",
      "natural emotion analysis",
      "unscripted emotion analysis"
    ]
  },
  {
    "topic_id": 115,
    "size": 12,
    "keywords": [
      "contextual emotion",
      "contextual",
      "contextaware emotion",
      "conversational context",
      "modeling conversational",
      "contextaware",
      "evolution",
      "context modeling",
      "context",
      "conversational"
    ],
    "examples": [
      "contextual emotion conveyance",
      "contextual emotion detection",
      "context-dependent emotion analysis",
      "contextual emotion estimation",
      "contextual call for help detection",
      "context-aware emotion classification",
      "context-aware emotion classification",
      "contextual emotion representation learning"
    ],
    "topic_fingerprint": "6f8e1049f0f0903fd128273fe28662947250ed3dd9eefa7e338da2caa1561f4c",
    "l2_name": "Contextual Emotion Modeling",
    "definition": "The computational task of detecting, classifying, or representing emotions by leveraging conversational and situational context to resolve ambiguity and track emotional evolution.",
    "aliases": [
      "context-aware emotion analysis",
      "context-dependent emotion detection",
      "conversational context modeling",
      "contextual emotion estimation",
      "contextual emotion representation learning",
      "conversational emotion context evolution",
      "context-aware emotion classification",
      "contextual emotion conveyance"
    ]
  },
  {
    "topic_id": 116,
    "size": 12,
    "keywords": [
      "interaction analysis",
      "dyadic",
      "interaction",
      "dyadic interaction",
      "analysis dyadic",
      "social interaction",
      "clinical dialog",
      "dialog",
      "dialog analysis",
      "interaction effects"
    ],
    "examples": [
      "identity-expression interaction effects",
      "expression-identity interaction effects",
      "dyadic interaction analysis",
      "dyadic interaction analysis",
      "dyadic dialogue analysis",
      "dyadic interaction analysis",
      "large group interaction analysis",
      "clinical dialog analysis"
    ],
    "topic_fingerprint": "90325bb7c078cd93518ac7c863bdca008e7d69b07610e5adffeedd754b05be95",
    "l2_name": "Dyadic Interaction Analysis",
    "definition": "The systematic examination of reciprocal behaviors, communication patterns, and mutual influences between two individuals within social or clinical contexts.",
    "aliases": [
      "dyadic interaction",
      "interaction analysis",
      "dyadic dialogue analysis",
      "clinical dialog analysis",
      "social interaction analysis",
      "dialog analysis",
      "analysis dyadic",
      "interaction effects",
      "identity-expression interaction effects",
      "expression-identity interaction effects"
    ]
  },
  {
    "topic_id": 117,
    "size": 12,
    "keywords": [
      "moral",
      "processing",
      "neural mechanisms",
      "stimuli",
      "emotional stimuli",
      "processing emotional",
      "mechanisms",
      "emotion processing",
      "impact",
      "cortex"
    ],
    "examples": [
      "differential processing of emotional stimuli",
      "emotional stimuli impact",
      "emotion processing brain mechanisms",
      "internal feeling processing",
      "moral behavior prediction",
      "moral emotion mechanism",
      "prefrontal cortex emotion processing",
      "emotional arousal impact"
    ],
    "topic_fingerprint": "5ce83de88b2b01aba89fbb1dbd3ffc1575d1353ed8120ff715798be0ef0629d2",
    "l2_name": "Neural Mechanisms of Moral and Emotional Processing",
    "definition": "This topic covers the identification and analysis of brain regions and neural pathways involved in processing emotional stimuli, moral judgments, and related internal feelings.",
    "aliases": [
      "moral emotion mechanism",
      "emotion processing neural mechanisms",
      "differential processing of emotional stimuli",
      "prefrontal cortex emotion processing",
      "neural basis of moral behavior",
      "emotional stimuli impact on brain",
      "embarrassment neural mechanisms",
      "moral conscience neural correlates",
      "brain mechanisms of emotion processing",
      "neural processing of moral stimuli"
    ]
  },
  {
    "topic_id": 118,
    "size": 12,
    "keywords": [
      "semantic",
      "logical",
      "logical coherence",
      "coherence",
      "poor",
      "semantic logic",
      "poor logical",
      "semantic feature",
      "logic",
      "semanticsaware"
    ],
    "examples": [
      "semantics-aware dynamic representation",
      "semantics-aware dynamic representation",
      "logical coherence improvement",
      "poor logical coherence",
      "poor logical coherence",
      "semantic understanding deficiency",
      "poor semantic logic",
      "semantic logic improvement"
    ],
    "topic_fingerprint": "2200e57b3756819fa3cb91f165b8d3616ba36645f72fc2c7dda02589c881e471",
    "l2_name": "Semantic Logical Coherence",
    "definition": "The deficiency in maintaining consistent logical relationships and meaningful semantic alignment within dynamic representations or feature structures.",
    "aliases": [
      "poor logical coherence",
      "semantic logic deficiency",
      "semantic feature consistency",
      "semantics-aware representation issues",
      "logical coherence improvement",
      "semantic alignment problems",
      "semantic feature separation",
      "affective semantic disentanglement"
    ]
  },
  {
    "topic_id": 119,
    "size": 12,
    "keywords": [
      "microgesture",
      "spontaneous microgesture",
      "microgesture based",
      "analysis microgesture",
      "microgesture analysis",
      "based emotion",
      "based",
      "microgesture recognition",
      "microgesture interpretation",
      "analysis spontaneous"
    ],
    "examples": [
      "spontaneous micro-gesture analysis",
      "spontaneous micro-gesture analysis",
      "spontaneous micro-gesture analysis",
      "micro-gesture based emotion analysis",
      "micro-gesture based emotion analysis",
      "micro-gesture based emotion analysis",
      "spontaneous micro-gesture interpretation",
      "spontaneous micro-gesture recognition"
    ],
    "topic_fingerprint": "db7256e64fe5eb0d3b7e3a9e67e7817d16c526d98175468d0bc2df4382e0a95a",
    "l2_name": "Spontaneous Micro-gesture Analysis",
    "definition": "The computational study and interpretation of involuntary, subtle facial or bodily movements to recognize human emotions and internal states.",
    "aliases": [
      "micro-gesture analysis",
      "spontaneous micro-gesture recognition",
      "micro-gesture based emotion analysis",
      "micro-gesture interpretation",
      "spontaneous micro-gesture understanding",
      "micro-gesture based emotion recognition",
      "analysis of spontaneous micro-gestures"
    ]
  },
  {
    "topic_id": 120,
    "size": 12,
    "keywords": [
      "hidden",
      "hidden emotion",
      "interpretation hidden",
      "analysis hidden",
      "hidden emotional",
      "interpretation",
      "state analysis",
      "emotion interpretation",
      "detection hidden",
      "emotional state"
    ],
    "examples": [
      "hidden emotion analysis",
      "hidden emotion detection",
      "hidden emotion interpretation",
      "hidden emotion interpretation",
      "hidden emotion interpretation",
      "hidden emotional state analysis",
      "hidden emotional state analysis",
      "hidden emotional state analysis"
    ],
    "topic_fingerprint": "08d1ec68bfdde1f01d547d0cd64a14f6f34518f02541e9a78ba19a144fc16c15",
    "l2_name": "Hidden Emotion Analysis",
    "definition": "The computational detection, interpretation, and analysis of concealed or unexpressed emotional states from behavioral or textual cues.",
    "aliases": [
      "hidden emotion detection",
      "hidden emotion interpretation",
      "hidden emotional state analysis",
      "inner feeling interpretation",
      "concealed emotion analysis",
      "latent emotion detection"
    ]
  },
  {
    "topic_id": 121,
    "size": 11,
    "keywords": [
      "correlation modeling",
      "correlation",
      "regional correlation",
      "modeling regional",
      "inter",
      "modeling",
      "regional",
      "intervideo correlation",
      "intramodal",
      "intracue"
    ],
    "examples": [
      "latent correlation modeling among signals",
      "inter- and intra-polarity modeling",
      "inter-video correlation modeling",
      "informative region modeling",
      "frequency domain correlation modeling",
      "intra-modal interaction modeling",
      "complex au correlation modeling",
      "inter- and intra-cue correlation modeling"
    ],
    "topic_fingerprint": "e0eee7ad4c2c3e927eb8986640dd5f291b225278b3b8ad77dd54513a98ccc329",
    "l2_name": "Correlation Modeling",
    "definition": "The computational process of capturing and leveraging statistical dependencies within and across different modalities, cues, regions, or signals to enhance representation learning.",
    "aliases": [
      "correlation modeling",
      "regional correlation modeling",
      "inter-video correlation modeling",
      "intra-modal interaction modeling",
      "inter-cue correlation modeling",
      "latent correlation modeling",
      "frequency domain correlation modeling",
      "inter- and intra-polarity modeling",
      "complex au correlation modeling",
      "informative region modeling"
    ]
  },
  {
    "topic_id": 122,
    "size": 11,
    "keywords": [
      "affective data",
      "scarcity",
      "scarcity emotion",
      "imbalance affective",
      "dataset scarcity",
      "class imbalance",
      "data scarcity",
      "data",
      "scarcity class",
      "affective dataset"
    ],
    "examples": [
      "affective data representation",
      "multimodal affective data scarcity",
      "data scarcity in affective computing",
      "data scarcity in emotion",
      "data scarcity in emotion",
      "class imbalance in affective data",
      "affective dataset scarcity",
      "emotion dataset scarcity"
    ],
    "topic_fingerprint": "20d6ae22b3f03209364c1480b6ce3631081ef707ef8a2fb22619338b5031c7e0",
    "l2_name": "Affective Data Scarcity",
    "definition": "The challenge of insufficient quantity or imbalanced distribution of labeled data available for training and evaluating affective computing models.",
    "aliases": [
      "emotion dataset scarcity",
      "class imbalance in affective data",
      "data scarcity in affective computing",
      "data scarcity in emotion",
      "multimodal affective data scarcity",
      "affective dataset scarcity",
      "imbalance affective",
      "dataset scarcity",
      "scarcity class",
      "scarcity emotion"
    ]
  },
  {
    "topic_id": 123,
    "size": 11,
    "keywords": [
      "annotation",
      "annotation handling",
      "inaccurate annotation",
      "ambiguity handling",
      "handling inaccurate",
      "inaccurate",
      "handling",
      "annotation ambiguity",
      "ambiguity",
      "insufficient annotation"
    ],
    "examples": [
      "annotation ambiguity handling",
      "weak annotation utilization",
      "high annotation cost",
      "expression ambiguity handling",
      "annotation ambiguity handling",
      "insufficient annotation data",
      "noisy annotation handling",
      "inaccurate annotation handling"
    ],
    "topic_fingerprint": "5e1638317ac1b05c32f712818460c8ba60dd6fe8718b9733ddb5d027c77d3c26",
    "l2_name": "Annotation Quality and Ambiguity",
    "definition": "This topic covers challenges related to inaccurate, insufficient, noisy, or ambiguous annotations and the methods developed to handle or mitigate these issues.",
    "aliases": [
      "annotation ambiguity handling",
      "inaccurate annotation handling",
      "noisy annotation handling",
      "insufficient annotation",
      "weak annotation utilization",
      "high annotation cost",
      "ambiguous expression annotation",
      "fixed label inaccuracy",
      "annotation error mitigation"
    ]
  },
  {
    "topic_id": 124,
    "size": 11,
    "keywords": [
      "modality",
      "mitigation modality",
      "modality imbalance",
      "contribution",
      "imbalance mitigation",
      "modality contribution",
      "imbalance",
      "unequal modality",
      "unequal",
      "mitigation"
    ],
    "examples": [
      "modality impact on awareness",
      "unequal modality contribution",
      "unequal modality contribution",
      "estimating channel contribution weights",
      "modality contribution analysis",
      "modality heterogeneity mitigation",
      "heterogeneous modality distribution gap",
      "modality imbalance mitigation"
    ],
    "topic_fingerprint": "54546e0e4b8e6656d8381833a4ccdfa02b5ebc1297c6cfd2dce2a8e2b824be6e",
    "l2_name": "Modality Imbalance",
    "definition": "The problem where different modalities contribute unequally to model performance, requiring mitigation strategies to balance their impact.",
    "aliases": [
      "modality bias",
      "unequal modality contribution",
      "modality heterogeneity",
      "modality imbalance mitigation",
      "modality contribution imbalance"
    ]
  },
  {
    "topic_id": 125,
    "size": 11,
    "keywords": [
      "microexpression database",
      "database",
      "scarcity microexpression",
      "database construction",
      "construction microexpression",
      "microexpression data",
      "construction",
      "scarcity",
      "data scarcity",
      "microexpression"
    ],
    "examples": [
      "micro-expression database scarcity",
      "micro-expression dataset scarcity",
      "micro-expression database construction",
      "micro-expression database construction",
      "micro-expression database construction",
      "micro-expression database construction",
      "micro-expression data scarcity",
      "micro-expression database bias"
    ],
    "topic_fingerprint": "d28a9317d155c310232e039a1823876082586815b4978ae2772b0430d2377d63",
    "l2_name": "Micro-expression Database Construction",
    "definition": "The process and challenges associated with creating, curating, and expanding datasets of micro-expressions to address issues of data scarcity and bias.",
    "aliases": [
      "microexpression database construction",
      "micro-expression dataset creation",
      "microexpression data collection",
      "database construction for micro-expressions",
      "micro-expression corpus building",
      "addressing micro-expression data scarcity",
      "micro-expression dataset scarcity"
    ]
  },
  {
    "topic_id": 126,
    "size": 11,
    "keywords": [
      "image retrieval",
      "retrieval",
      "retrieval task",
      "affective image",
      "image",
      "emotionbased image",
      "task affective",
      "retrieval emotionbased",
      "retrieval affective",
      "emotionbased"
    ],
    "examples": [
      "affective music-image retrieval",
      "affective image retrieval task",
      "emotion-based image search",
      "affective image retrieval task",
      "affective image retrieval",
      "efficient image retrieval",
      "affective image retrieval task",
      "affective image retrieval task"
    ],
    "topic_fingerprint": "b941601a8d0bc0fbaebffdc120422ade533842a8c92025d57a64aa6a44ea2a28",
    "l2_name": "Affective Image Retrieval",
    "definition": "The task of retrieving images from a database based on the emotional content or affective response they evoke in users.",
    "aliases": [
      "emotion-based image retrieval",
      "affective image search",
      "emotion-based image search",
      "image retrieval by emotion",
      "affective retrieval",
      "emotion-driven image retrieval"
    ]
  },
  {
    "topic_id": 127,
    "size": 11,
    "keywords": [
      "ambiguity",
      "ambiguity resolution",
      "resolution",
      "emotional ambiguity",
      "sentiment ambiguity",
      "emotion ambiguity",
      "resolution emotion",
      "ambiguity modeling",
      "ambiguity multimodal",
      "perception ambiguity"
    ],
    "examples": [
      "emotion perception ambiguity modeling",
      "emotion perception ambiguity resolution",
      "emotion ambiguity resolution",
      "emotion ambiguity resolution",
      "multimodal sentiment ambiguity",
      "multimodal sentiment ambiguity",
      "multimodal sentiment ambiguity",
      "emotion label ambiguity"
    ],
    "topic_fingerprint": "b3fa1bb2d579f413b5e71d6207ffa5a9122d04b4fcdf0cfff41eb91a8c4164ad",
    "l2_name": "Emotional Ambiguity Resolution",
    "definition": "The computational modeling and disambiguation of conflicting or unclear emotional signals within text, speech, or multimodal data to determine precise sentiment.",
    "aliases": [
      "emotion ambiguity resolution",
      "sentiment ambiguity resolution",
      "emotional ambiguity modeling",
      "ambiguity in emotion perception",
      "multimodal sentiment ambiguity",
      "emotion label ambiguity",
      "resolution of emotional ambiguity"
    ]
  },
  {
    "topic_id": 128,
    "size": 10,
    "keywords": [
      "emotional state",
      "state understanding",
      "holistic emotional",
      "holistic",
      "understanding holistic",
      "state",
      "understanding",
      "emotional",
      "detection detecting",
      "detecting emotional"
    ],
    "examples": [
      "emotional state detection",
      "emotional state classification",
      "emotional state dynamics analysis",
      "holistic emotional state understanding",
      "holistic emotional state understanding",
      "holistic emotional state understanding",
      "emotional orientation detection",
      "detecting emotional shift issues"
    ],
    "topic_fingerprint": "d998b32846a4124c6aad85af2c82c889633e06f760de711471a9440bcc5d2165",
    "l2_name": "Emotional State Understanding",
    "definition": "The comprehensive detection, classification, and holistic analysis of an individual's emotional condition and its dynamics.",
    "aliases": [
      "emotional state detection",
      "emotional state classification",
      "holistic emotional state understanding",
      "emotional orientation detection",
      "detecting emotional shift issues",
      "emotional engagement detection",
      "open-vocabulary emotion state description",
      "state understanding",
      "detecting emotional"
    ]
  },
  {
    "topic_id": 129,
    "size": 10,
    "keywords": [
      "negative",
      "negative emotion",
      "emotion impact",
      "negative mood",
      "impact",
      "style mitigation",
      "relief",
      "assessment negative",
      "bereavementrelated",
      "cognition negative"
    ],
    "examples": [
      "negative mood relief",
      "positive emotion reduction",
      "negative mood regulation",
      "negative emotion impact assessment",
      "negative coping style mitigation",
      "negative emotion discrimination impairment",
      "bereavement-related immune alteration",
      "negative emotion impact on cognition"
    ],
    "topic_fingerprint": "71b9d005ae5cc310208eb7adc55b59e97febac596dd738599e4bc0ca56ce59d2",
    "l2_name": "Negative Emotion Impact and Regulation",
    "definition": "This topic encompasses the assessment of adverse effects caused by negative emotions on cognition and physiology, as well as strategies for their mitigation, relief, and regulation.",
    "aliases": [
      "negative mood regulation",
      "negative emotion impact assessment",
      "negative coping style mitigation",
      "negative emotion alleviation",
      "negative mood relief",
      "bereavement-related immune alteration",
      "negative emotion discrimination impairment",
      "impact of negative emotions on cognition",
      "negative emotion filtering"
    ]
  },
  {
    "topic_id": 130,
    "size": 10,
    "keywords": [
      "dysfunction",
      "emotional dysfunction",
      "adolescent",
      "regulation",
      "emotional dysregulation",
      "adolescent emotional",
      "disorder regulation",
      "characterization adolescent",
      "developmental emotion",
      "deficit characterization"
    ],
    "examples": [
      "emotional self-regulation disturbances",
      "emotional dysfunction mechanisms",
      "ptsd emotional dysfunction",
      "social-emotional deficit characterization",
      "adolescent emotional well-being analysis",
      "taste dysfunction correlation",
      "emotional dysregulation mechanisms",
      "post-stroke emotional disorder regulation"
    ],
    "topic_fingerprint": "4a015f71b21265b3b6d2552a989d703b4ead36d40bf29c4bf7538427ad63ff3a",
    "l2_name": "Emotional Dysregulation",
    "definition": "Emotional dysregulation refers to the impaired ability to monitor, evaluate, and modify emotional reactions in a manner appropriate to the context, often observed in adolescents and various psychological disorders.",
    "aliases": [
      "emotional dysfunction",
      "emotion regulation deficits",
      "affective dysregulation",
      "emotional self-regulation disturbances",
      "developmental emotion risks",
      "adolescent emotional dysregulation",
      "disorder regulation",
      "social-emotional deficit characterization"
    ]
  },
  {
    "topic_id": 131,
    "size": 10,
    "keywords": [
      "realtime",
      "realtime emotion",
      "detection realtime",
      "performance optimization",
      "optimization realtime",
      "realtime performance",
      "monitoring realtime",
      "emotion monitoring",
      "optimization",
      "negative emotion"
    ],
    "examples": [
      "collective emotion monitoring",
      "real-time negative emotion detection",
      "real-time performance optimization",
      "real-time performance optimization",
      "real-time emotion recognition",
      "emotion arousal state analysis",
      "negative emotion detection",
      "real-time emotion monitoring"
    ],
    "topic_fingerprint": "76ca18bdb6e3880fbeea1391c0bce0349f7a54140c27abf09ad7429d3b193241",
    "l2_name": "Real-time Emotion Monitoring",
    "definition": "The continuous detection, analysis, and optimization of emotional states, particularly negative emotions, in real-time environments.",
    "aliases": [
      "realtime emotion detection",
      "real-time negative emotion detection",
      "realtime emotion recognition",
      "emotion arousal state analysis",
      "realtime performance optimization",
      "collective emotion monitoring",
      "real-time emotion fitting",
      "emotion monitoring"
    ]
  },
  {
    "topic_id": 132,
    "size": 10,
    "keywords": [
      "quantification",
      "quantification microexpression",
      "microexpression quantification",
      "microexpression",
      "difficulty microexpression",
      "duration measurement",
      "microexpression duration",
      "microexpression characteristic",
      "annotation microexpression",
      "characteristic validation"
    ],
    "examples": [
      "micro-expression duration measurement",
      "micro-expression quantification",
      "micro-expression quantification",
      "micro-expression quantification",
      "micro-expression intensity quantification",
      "micro-expression characteristic validation",
      "micro-expression annotation difficulty",
      "micro-expression data annotation"
    ],
    "topic_fingerprint": "7438280a1e6973e3530e98c50a5d8cb23d4357d669c09434550ace9d2758ada5",
    "l2_name": "Microexpression Quantification",
    "definition": "The measurement and validation of microexpression characteristics including duration, intensity, and annotation difficulty.",
    "aliases": [
      "micro-expression quantification",
      "quantification of microexpressions",
      "microexpression duration measurement",
      "microexpression intensity quantification",
      "microexpression characteristic validation",
      "microexpression annotation difficulty",
      "ME quantification"
    ]
  }
]
